
\section{Wednesday}\index{week7_Thursday_lecture}
Given the optimization problem
\[
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&x\in X\mbox{ is convex}
\end{array}
\]
The necessary optimality condition is
\[
\nabla\trans f(x^*)(x-x^*)\ge0,\qquad
\forall x\in X
\]

If we know that $x^*$ is exactly interior to the convex set, we have $\nabla f(x^*)=0$; if $x^*$ is on the boundary of the convex set, then the gradient should be perpendicular to the convex set, i.e., $\nabla f(x^*)$ should be orthogonal to the tangent line at $x=x^*$.
\begin{enumerate}
\item
If $X$ is affie, then
\[
\nabla f(x^*)\perp X.
\]
\end{enumerate}
\paragraph{Projection Study}Given $z$, we aim to solve
\[
\begin{array}{ll}
\min&\frac{1}{2}\|x-z\|_2^2\\
\mbox{such that}&x\in X\mbox{is convex and closed}
\end{array}
\]
Here the optimal solution is denoted as $x^*=[z]^+=\Proj_X(z)$. We have $\nabla f(x) = x-z$, thus the necessary conditon for local minimum of $x^*$ should be
\[
(x^*-z)\trans(x-x^*)\ge0,\qquad\forall x\in X
\]
Or equivalently,
\begin{equation}
(z-[z]^+)\trans(x-[z]^+)\le0,\qquad\forall x\in X.\label{Eq:7:2}
\end{equation}
Since the objective function is convex, (\ref{Eq:7:2}) is both necessary and sufficient condition.

The non-expansive property guarantee that $[z]^+$ is unique for any $z$ (over convex set), since otherwise
\[
0\le\|[z]^+_1-[z]^+_2\|\le\|z-z\|=0.
\]
\begin{example}
\begin{enumerate}
\item
For $X=\{x|x\ge0\}$, we have
\[
[z]^+_i=\max(0,z_i),\qquad i=1,2
\]
\item
For $X=\{\bm X\in\mathbb{S}^n|\bm X\succeq0\}$, every element admits eigen-decomposition:
\[
\bm X=\bm Q\bm{\Lambda}\bm Q\trans.
\]
If we want to minimize $\|\bm X-\bm Z\|_F^2$ for given $\bm Z$, the projection
\[
[\bm Z]^+=\bm Q\max(\bm0,\bm\Lambda)\bm Q\trans
\]
\item
For the \emph{ellipsoid set} $X=\{\bm x|\bm x\trans\bm Q\bm x\le1\}$ with $\bm Q\succ0$, we aim to minimize $\|\bm x-\bm z\|_2^2$ for given $\bm z$:
\[
\bm x\trans\bm Q\bm x=(\bm U\trans\bm x)\trans\bm\Lambda(\bm U\trans\bm x):=\bm y\trans\bm\Lambda\bm y.
\]
Thus $\|\bm x-\bm z\|_2=\|\bm U\trans(\bm x-\bm z)\|_2=\|\bm y-\bar{\bm z}\|_2$. The problem just becomes
\[
\begin{array}{ll}
\min&\|\bm y-\bar{\bm z}\|\\
\mbox{such that}&\bm y\in Y=\{\bm y|\bm y\trans\bm\Lambda\bm y\le1\}
\end{array}
\]
Assume $\bar{\bm z}\notin Y$, then it suffices to solve
\[
\begin{array}{ll}
\min&\|\bm y-\bar{\bm z}\|_2^2\\
\mbox{such that}&\bm y\trans\bm\Lambda\bm y=1
\end{array}
\]
Define $L(y,\lambda)=\|\bm y-\bar{\bm z}\|_2^2+\lambda(\bm y\trans\bm\Lambda\bm y-1)$ ($\lambda\ge0$), the necessary condition is
\[
\nabla L(y,z)=0\implies
\left\{
\begin{aligned}
2(\bm y-\bar{\bm z})+2\lambda\bm\Lambda\bm y&=0\\
\bm y\trans\bm\Lambda\bm y-1&=0
\end{aligned}
\right.
\]
The first equation gives $(\bm I+\lambda\bm\Lambda)\bm y=\bm z\implies\bm y=(\bm I+\lambda\bm\Lambda)^{-1}\bar{\bm z}$, and therefore
\[
\bar{\bm z}\trans(\bm I+\lambda\bm\Lambda)^{-1}\bm\Lambda(\bm I+\lambda\bm\Lambda)^{-1}\bar{\bm z}=1
\]
Define $\bm\Lambda=\diag(\mu_1,\dots,\mu_n)\succ0$, we can solve for $\lambda$:
\[
\sum\frac{\bar{z_i}^2\mu_i}{(1+\lambda\mu_i)^2}=1
\]
For the spaecial case $\mu_i\equiv\mu$, we have
\[
y=(1+\lambda\mu)^{-1}z,\qquad
\lambda=\frac{1}{\mu}(\sqrt{\mu}\|\bm z\|-1)
\]
\end{enumerate}
\end{example}
\subsection{Convex Analysis}
For convex set $X$ and convex function $f$, consider the problem
\[
\begin{array}{ll}
\min&f(x)\in\mathcal{C}^1\\
\mbox{such that}&x\in X
\end{array}
\]
the stationary of $x^*$ is equivalent to
\[
\inp{\nabla f(x^*)}{(x-x^*)}\ge0,\forall x\in X
\]
which is equivalent to
\[
x^*=[x^*-\alpha\nabla f(x^*)]^+,\quad \forall\alpha>0
\]
\begin{proof}
Let $z=x^*-\alpha\nabla f(x^*)$, and consider
\begin{align*}
\min_{x}\|x-z\|^2&=\|x-x^*+\alpha\nabla f(x^*)\|_2^2\\
&=\|x-x^*\|^2+2\alpha\inp{\nabla f(x^*)}{(x-x^*)}+\alpha^2\|\nabla^2f(x^*)\|^2
\end{align*}
which implies the minimum point is $x^*$, i.e., $x^*=[z]^+$.
\end{proof}
\paragraph{Algorithm for constraint minimization}
The idea for feasible direction methods is that:
\begin{quotation}
Generate $\{x^k\}\subseteq X$ such that
\[
f(x^{r+1})\le f(x^r)
\]
\end{quotation}
Or equivalently, find $\bar{x}^r\in X$ such that
\[
\inp{\nabla f(x^r)}{(\bar{x}^r-x^r)}<0
\]
and then
\[
x^{r+1}=x^r+\alpha_r(\bar{x}^r-x^r),\qquad
\alpha_r\in(0,1]
\]
Since $x^{r+1}$ is a convex combination of $x^r$ and $\bar{x}^r$, it must be feasible.
\begin{enumerate}
\item
One way for finding $\bar{x}^r$ is that find $x^r-\alpha\nabla f(x^r)$ and project it back as $\bar{x}^r$. (\emph{gradient projection}):
\[
\bar{x}^r=[x^r-s_r\nabla f(x^r)]^+,
\]
with $s_r>0$. Therefoer $x^{r+1} = x^r + \alpha_r(\bar{x}^r-x^r)$
\item
Another way is called the \emph{conditional gradient}/Frank-wolfe:
\[
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&x\in X
\end{array}\approx
\bar{x}^r=\arg\min_{x\in X}f(x^r)+\inp{\nabla f(x^r)}{(x-x^r)}
\]
and therefore $x^{r+1}=x^r+\alpha_r(\bar{x}^r-x^r)$


\end{enumerate}â€¢











