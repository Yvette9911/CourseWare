
\chapter{Week3}

\section{Wednesday}\index{week3_Tuesday_lecture}
Assignment 2 posted.

CIE6010: Exercise 1.2.9 and 1.3.9; together with MATLAB project.

\subsection{Convex Analysis}
For the \emph{constrained} problem
\[
\begin{array}{ll}
\min&\quad f(x)\\
&\bm x\in X\subseteq\mathbb{R}\\
&f\mbox{ is convex in }\mathcal{C}^1
\end{array}
\]
$\bm x$ is a global minimum iff $\nabla\trans f(\bm x)(\bm y - \bm x)\ge0$ for $\forall y\in X$

Such a condition is not useful unless $\bm y$ lies in the whole space, at that time we have no choice but $\nabla f(\bm x)=\bm0$.

An equivalent version of the condition is that every \emph{feasible} direction is \emph{ascending}.

\begin{definition}[Descending Direction]
$\bm d\in\mathbb{R}^n$ is a \emph{descending direction} of $f$ at $\bm x$ if
\[
\nabla\trans f(\bm x)\bm d<0.
\]
\end{definition}
This definition is the motivation of descent method.

\subsection{Iterative Method}
\begin{definition}[Descent Method]
At ant non-stationary $\bm x$, i.e., $\nabla f(x)\ne\bm0$, we find descent $d$, i.e., $\nabla\trans f(\bm x)\bm d<0$. We update our old $\bm x$ as:
\[
\bm x^{r+1}\leftarrow \bm x^r+\alpha^r\bm d^r,\quad \alpha>0.
\]
\end{definition}
The key is how to chooose $\bm d$ and $\alpha$. We have a general formula for $\bm d$, which is the \emph{descent direction}:
\[
\bm d = -\bm D\cdot\nabla f(\bm x),
\]
where $\bm D\in\mathbb{S}^n$ and $\bm D\succ0$.

e.g., $\bm D=\bm I$ implies gradient method (Steepest Descent).

$\bm D = \left(\nabla^2 f(\bm x)\right)^{-1}$ implies the Newton's method.

\paragraph{Nonlinear LS} The optimization problem is
\[
\begin{array}{ll}
\min&\quad f(\bm x) = \frac{1}{2}\sum_{i=1}^mg_i^2(\bm x):=\frac{1}{2}\|g(\bm x)\|_2^2\\
&g(\bm x)=\begin{pmatrix}
g_1(\bm x)&g_2(\bm x)&\cdots&g_m(\bm x)
\end{pmatrix}\trans
\end{array}
\]

The gredient function is
\[
\nabla f(\bm x)=\sum_{i=1}^mg_i(\bm x)\nabla g_i(\bm x)
=
\underbrace{\begin{bmatrix}
\nabla g_1(\bm x)&\cdots&\nabla g_m(\bm x)
\end{bmatrix}}_{\nabla g(x)\in\mathbb{R}^{n\times m}}\begin{bmatrix}
g_1(\bm x)\\\vdots\\ g_m(\bm x)
\end{bmatrix}=
\nabla g(\bm x)\cdot g(\bm x)=\bm J\trans(\bm x)g(\bm x),
\]
where $J(\bm x)\in\mathbb{R}^{m\times n}$ is the Jacobian of $g$.

The second order derivative is given as:
\[
\nabla^2 f(\bm x) = \bm J\trans(\bm x)\bm J(\bm x)+\sum_{i=1}^mg_i(\bm )\nabla^2 g_i(\bm x)
\]
the second term in RHS is complicated and hard to compute. The Gauss-Newton method directly ignore it.
\paragraph{Choice of Step Length $\alpha$}
Exact line search
\[
\min_{\alpha\in(0,M]}f(\bm x^r+\alpha^r\bm d^r)
\]
usually it is too expensive. Sometimes we choose $\alpha$ as a constant.
\begin{definition}[Lipschitz Continuous]
$\nabla f$ is \emph{Lipschitz continuous} with Lipschitz constant $L$ if
\[
\|\nabla f(\bm x) - \nabla f(\bm y)\|\le L\|\bm x-\bm y\|
\]
for all $\bm x,\bm y$.
\end{definition}
\begin{remark}
This condition guarantees that 
\[
f(\bm x^r) - f(\bm x^{r+1})\ge\frac{L}{2}\|\nabla f(\bm x^r)\|^2
\]
From this inequality, we imply that the result of convergence is $\nabla f(\bm x^r)\to0$, but the minimum point is still un-guaranteed. In Deep Learing people often train the data using this way, which is not so rigorous.
\end{remark}
We set $h(t) = f(\bm x+t\alpha\bm d)$, then we can use Lipschitz continuous to analysis the rate of convergence and the choice of step-length:
\begin{align*}
f(\bm x+\alpha\bm d) - f(\bm x)
&=
\int_0^1\nabla\trans f(\bm x+t\cdot\alpha\bm d)\alpha d\diff t\\
&=
\int_0^1\left[
\nabla\trans f(\bm x+t\cdot\alpha\bm d)\alpha\bm d
-\nabla\trans f(\bm x)\alpha\bm d
+\nabla\trans f(\bm x)\alpha\bm d
\right]\\
&\le
\nabla\trans f(\bm x)\alpha\bm d
+
\int_0^1\|\nabla f(\bm x+\alpha\bm d) - \nabla f(\bm x)\|\cdot\|\alpha\bm d\|\diff t\\
&\le \nabla\trans f(\bm x)\alpha\bm d
+L\int_0^1t\alpha^2\|d\|^2\diff t\\
&=\underbrace{\nabla\trans f(\bm x)\alpha\bm d}_{\text{negative}}+\frac{L\alpha^2\| d\|^2}{2}
\end{align*}

Differentiate the RHS w.r.t. $\alpha$ leads to
\[
\nabla\trans f(\bm x)\bm d+L\alpha\|\bm d\|^2=0\implies
\alpha=-\frac{\nabla\trans f(\bm x)\bm d}{L\|\bm d\|^2}>0,
\]
which seems a reasonable choice. If $\bm d$ is the steepest descent direction, the step-length becomes:
\[
\alpha=\frac{1}{L}\mbox{ (constant)}
\]












