
\chapter{Week4}

\section{Wednesday}\index{week4_Wednesday_lecture}
You need to take care of several things during your assignment:
\begin{enumerate}
\item
Do Not Repeat Computation! e.g., if you enter
\[
|f(x^{k+1}) - f(x^k)|\le 10^{-\varepsilon}|f(x^k)|,
\]
this is very bad, since you evaluated this function three times.
\item
Arrange Computation Properly. e.g., compute
\[
\bm x\bm x\trans\bm A
\]
is very expensive, but $\bm x(\bm x\trans\bm A)$ is not. Be aware of the size of matrices / vectors.
\item
Appreciate sparsity, e.g., to compute $\bm A\bm D\bm A\trans$ is bad if using $\mathrm{D = diag(d)}$, while using $\mathrm{D = sparse(1:n,1:n,d)}$ is better.
\end{enumerate}
Your code should be at least faster than the testing script.

\subsection{Local Convergence Rate}
\begin{definition}[$Q_1$ Factor]
Let $x^k\to x^*$ and $e_k := \|x^k - x^*\|\to0$. The $Q_1$ factor of $\{x^k\}$ is given as:
\[
Q_1 = \lim_{k\to\infty}\sup\frac{\|\bm e^{k+1}\|}{\|\bm e^k\|}
\]
\end{definition}
\begin{itemize}
\item
The sequence $\{x^k\}$ is convergent if $Q_1\in[0,1]$.
\item
\[
Q_1=\left\{
\begin{aligned}
0,&\quad\mbox{Q-super linear convergence}\\
\rho\in(0,1),&\quad\mbox{Q-linear convergence}\\
1,&\quad\mbox{Q-sublinear}
\end{aligned}
\right.
\]
\end{itemize}
\begin{remark}
Q-linear convergence is not always so good, e.g., $\rho=.999$ may require 20000 iterations, while $\rho=.1$ may only require 20.

Linear is always better than sublinear; super linear is always btetter than linear. e.g.,
\[
\{e_1^k\}=\{\beta^k\},\qquad
\{e_2^k\} = \{\frac{1}{k^p}\}\mbox{ for $p=1/2,1,2$}
\]
Then $Q_1(e_1^k)=\beta\in(0,1)$; and $Q_1(e_2^k)=\lim_{k\to\infty}\frac{(k+1)^p}{k^p}=1$. When will $e^k\le\varepsilon$?
\begin{align*}
\beta^k&\le\varepsilon\implies k\ge(\frac{-1}{\ln\beta})\ln\frac{1}{\varepsilon} = O(\ln\frac{1}{\varepsilon})\\
\frac{1}{k^p}&\le\varepsilon\implies
k\ge(\frac{1}{\varepsilon})^{1/p}
\end{align*}
If $\varepsilon=10^{-8}$, $O(\ln\frac{1}{\varepsilon})$ is not growing very fast. For $p=1$ case, $k\ge O(\frac{1}{\varepsilon})$
\end{remark}

We have a faster type of convergence:
\begin{definition}[$Q_2$ Factor]
\[
Q_2 = \lim\sup_{k\to\infty}\frac{e^{k+1}}{(e^k)^2}
\]
If $Q_2=M<+\infty$,i.e., $e^{k+1}=O((e^k)^2)$, then $\{x^k\}\to x^*$ $Q$-quadratically.
\end{definition}
Newton's method generally gives us the quadratic convergence.

\subsection{Newton's Method}
The newton's method requires to solve a non-linear system of equations: ($\nabla f(x)=0$)
\[
F:\mathbb{R}^n\to\mathbb{R}^n, F(x)=0.
\]
We don't know how to solve non-linear system in general. But Newton gives us the remediation: do the linearization. 
\[
F(x+d)\approx F(x)+\inp{F(x)}{\bm d}=0
\]
To get the solution, it suffices to solve
\[
\bm d=-(F'(\bm x))^{-1}F(\bm x),
\]
and hence $\bm x\leftarrow \bm x+\alpha\bm d$.
\paragraph{Algorithm} Choose $\bm x^0$.

For $k=0,1,2,\dots$, solve the system w.r.t. $\bm d$:
\[
\nabla^2 f(\bm x^k)\bm d = -\nabla f(\bm x^k)
\]

Set $\bm x^{k+1} = \bm x^k + \alpha^k\bm d^k$.

End.
\paragraph{Interpretation}
In order to minimize a strictly convex function $f(x)$, we find
\[
f(\bm x+\bm d) \approx f(\bm x)+\inp{\nabla f(\bm x)}{\bm d}+\frac{1}{2}\bm d\trans\nabla^2 f(\bm x)\bm d:=q(\bm d)
\]
It suffices to minimize $q(\bm d)$:
\[
\nabla q(\bm d)=0\Longleftrightarrow
\nabla f(\bm x)+\nabla^2f(\bm x)\bm d=0
\]
How to guarntee $\bm d$ is the descent direction? It becomes an art.

\paragraph{Convergence of Rate}
For $F(x)=0$, suppose$\{\bm x^k\}$ is generated by Newton. As $x^k\to x^*$, $F(x^*)=0$. From
\[
x^{k+1} = x^k - [F'(x^k)]^{-1}F(x^k),
\]
we find
\begin{align*}
x^{k+1} - x^* &=  x^k - x^* -  [F'(x^k)]^{-1}\left(F(x^k) - F(x^*)\right)\\
&=[F'(x^k)]^{-1}\left(F(x^*) - F(x^k) - F'(x^k)(x^* - x^k)\right)
\end{align*}
It follows that
\[
\|\bm x^{k+1} - \bm x^*\|\le \|[F'(x^k)]^{-1}\|O(\|\bm x^k - \bm x^*\|^2)
=
O(\|\bm x^k - \bm x^*\|^2)
\]
During this deviation we assume two things:
\begin{itemize}
\item
Step-size is $1$!
\item
The limit exists.
\end{itemize}
Hence, in practice, to implement Newton's method, $1$ is the first choice; but gredient descent method is not.

Newton's method is good for nice problems, i.e., the funciton is convex, and the inverse of gredient is easy to solve.

In machine learning most time we implement the gredient descent method. 

Learn and implement these things by yourself:
\begin{itemize}
\item
Luo's note: Lecture $\#4$, P7, Nestorov Accelerated Method
\end{itemize}
$\{a_r\}, r=0,1,2,\dots$ we have
\[
a_r = \frac{1}{2}(1+\sqrt{1+4a_{r-1}^2})\mbox{ with }a_0=0
\]
From $a_r$ we generate $t^r = (a_{r}-1) / a_{r+1}$
\paragraph{Algorithm}
Set $x^0 = x^1=0$.

for $r=1,2,\dots$

compute $a^{r+1},t^r$

$\bm y^{r+1} = (1+t^r)\bm x^r - t^r\bm x^{r-1}$

$\bm x^{r+1} = y^{r+1} - \frac{1}{L}\nabla f(\bm y^{r+1})$

end














