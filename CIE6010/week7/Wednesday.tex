
\section{Wednesday}\index{week7_Thursday_lecture}
\subsection{Motivation}
Given the optimization problem
\begin{equation}\label{Eq:7:2}
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&x\in X\mbox{ is convex}
\end{array}
\end{equation}
Recall that The necessary optimality condition for (\ref{Eq:7:2}) is
\[
\begin{array}{ll}
\inp{\nabla f(x^*)}{(x-x^*)}\ge0
&
\forall x\in X
\end{array}
\]
\begin{remark}
\begin{itemize}
\item
If we know $x^*$ is exactly interior to the convex set, we have $\nabla f(x^*)=0$;
\item
If $x^*$ is on the boundary of the convex set, then the gradient should be perpendicular to the convex set, i.e., $\nabla f(x^*)$ should be orthogonal to the tangent line at $x=x^*$. 
\item
In particular, if $x^*\in\partial X$ with $X$ to be an affine, then 
\[
\nabla f(x^*)\perp X.
\]
\end{itemize}
\end{remark}
\paragraph{Motivation of Feasible direction method}Recall that the idea of gradient descent is to find a direction $\bm d^k$ at the point $\bm x^k$ such that $\inp{\nabla f(x^k)}{\bm d^k}<0$ and therefore $\bm x^{k+1}=\bm x^k+\bm d^k$; However, sometimes in constraint optimization problem (\ref{Eq:7:2}) the update position $\bm x^{k+1}$ may not be necessarily feasible. To handle such a barrier, we can try two algorithms: conditional gradient and projected gradient descent method. Before it let's study some preliminaries about projection. 
\subsection{Convex Projections}
The \emph{projection} step aims to solve the optimization problem defined as follows: 
\begin{definition}[Projection]
for given $z$, define the projection as
\begin{equation}\label{Eq:7:3}
\begin{array}{ll}
\min&\frac{1}{2}\|x-z\|_2^2\\
\mbox{such that}&x\in X\mbox{ is convex and closed},
\end{array}
\end{equation}
where $[z]^+:=\arg\min_{x}\frac{1}{2}\|x-z\|_2^2$ is called the \emph{projection operator}. Sometimes we also write $\arg\min_{x}\frac{1}{2}\|x-z\|_2^2$ as $\Proj_{X}(z)$. 
\end{definition}
\begin{proposition}[Zero order projection property]
Given any set $X\subseteq\mathbb{R}^n$ (may not necessarily convex) and $z\in\mathbb{R}^n$, for any $x\in X$, we have $\|[z]^+-z\|_2\le\|x-z\|_2$
\end{proposition}
The proof is due to the optimality of $[z]^+$.
\begin{proposition}[First order projection property]
Given \emph{convex} set $X$, the necessary and sufficient condition for the local minimum $x^*$ for problem (\ref{Eq:7:3}) is:
\begin{equation}\label{Eq:7:4}
\begin{array}{ll}
\inp{x-[z]^+}{z-[z]^+}\le0,&\forall x\in X
\end{array}
\end{equation}
\end{proposition}
\begin{proof}
Define $f(x):=\frac{1}{2}\|x-z\|_2^2$, thus $\nabla f(x)=x-z$, with the necessary condition of local minimum $x^*$ as:
\[
\inp{\nabla f(x^*)}{(x-x^*)}\ge0,\quad\forall x\in X\implies
\inp{x-[z]^+}{z-[z]^+}\le0.
\]
The sufficency of this condition is due to the convexity of problem (\ref{Eq:7:3}).
\end{proof}

\begin{proposition}[Non-expansive]
\[
\|[\bm z_1]^+ - [\bm z_2]^+\|\le\|\bm z_1-\bm z_2\|
\]
with $X$ to be a convex set.
\end{proposition}
\begin{proof}
Recall the first order property on $z_1,z_2$, we obtain
\[
\left\{
\begin{aligned}
\inp{z_1-[z_1]^+}{x-[z_1]^+}&\le0,\forall x\in X\\
\inp{z_2-[z_2]^+}{x-[z_2]^+}&\le0,\forall x\in X
\end{aligned}
\right.\implies
\left\{
\begin{aligned}
\inp{z_1-[z_1]^+}{[z_2]^+-[z_1]^+}&\le0,\forall x\in X\\
\inp{z_2-[z_2]^+}{[z_1]^+-[z_2]^+}&\le0,\forall x\in X
\end{aligned}
\right.
\]
Adding the inequalities above, we derive
\[
\inp{z_1-z_2+[z_2]^+-[z_1]^+}{[z_2]^+-[z_1]^+}\le0
\implies
\inp{[z_2]^+-[z_1]^+}{[z_2]^+-[z_1]^+}\le
\inp{z_2-z_1}{[z_2]^+-[z_1]^+}
\]
Applying Cauchy Scharwz inequality, we obtain:
\[
\|[z_2]^+-[z_1]^+\|_2^2\le \inp{z_2-z_1}{[z_2]^+-[z_1]^+}\le
\|z_2-z_1\|\|[z_2]^+-[z_1]^+\|
\]
Or equivalently, $\|[z_2]^+-[z_1]^+\|\le \|z_2-z_1\|\|$.
\end{proof}
\begin{remark}
The non-expansive property guarantee that $[z]^+$ is unique for any $z$ (over convex set), since otherwise
\[
0\le\|[z]^+_1-[z]^+_2\|\le\|z-z\|=0.
\]
\end{remark}


\begin{example}
\begin{enumerate}
\item
For $X=\{x|x\ge0\}\subseteq\mathbb{R}^2$, we have
\[
[z]^+_i=\max(0,z_i),\qquad i=1,2
\]
\item
For $X=\{\bm X\in\mathbb{S}^n|\bm X\succeq0\}$, every element admits eigen-decomposition:
\[
\bm X=\bm Q\bm{\Lambda}\bm Q\trans.
\]
If we want to minimize $\|\bm X-\bm Z\|_F^2$ for given $\bm Z$, the projection
\[
[\bm Z]^+=\bm Q\max(\bm0,\bm\Lambda)\bm Q\trans
\]
\item
For the \emph{ellipsoid set} $X=\{\bm x|\bm x\trans\bm Q\bm x\le1\}$ with $\bm Q\succ0$, we aim to minimize $\|\bm x-\bm z\|_2^2$ for given $\bm z$:
\[
\bm x\trans\bm Q\bm x=(\bm U\trans\bm x)\trans\bm\Lambda(\bm U\trans\bm x):=\bm y\trans\bm\Lambda\bm y.
\]
Thus $\|\bm x-\bm z\|_2=\|\bm U\trans(\bm x-\bm z)\|_2=\|\bm y-\bar{\bm z}\|_2$. The problem just becomes
\[
\begin{array}{ll}
\min&\|\bm y-\bar{\bm z}\|\\
\mbox{such that}&\bm y\in Y=\{\bm y|\bm y\trans\bm\Lambda\bm y\le1\}
\end{array}
\]
Assume $\bar{\bm z}\notin Y$, then it suffices to solve
\[
\begin{array}{ll}
\min&\|\bm y-\bar{\bm z}\|_2^2\\
\mbox{such that}&\bm y\trans\bm\Lambda\bm y=1
\end{array}
\]
Define $L(y,\lambda)=\|\bm y-\bar{\bm z}\|_2^2+\lambda(\bm y\trans\bm\Lambda\bm y-1)$ ($\lambda\ge0$), the necessary condition is
\[
\nabla L(y,z)=0\implies
\left\{
\begin{aligned}
2(\bm y-\bar{\bm z})+2\lambda\bm\Lambda\bm y&=0\\
\bm y\trans\bm\Lambda\bm y-1&=0
\end{aligned}
\right.
\]
The first equation gives $(\bm I+\lambda\bm\Lambda)\bm y=\bm z\implies\bm y=(\bm I+\lambda\bm\Lambda)^{-1}\bar{\bm z}$, and therefore
\[
\bar{\bm z}\trans(\bm I+\lambda\bm\Lambda)^{-1}\bm\Lambda(\bm I+\lambda\bm\Lambda)^{-1}\bar{\bm z}=1
\]
Define $\bm\Lambda=\diag(\mu_1,\dots,\mu_n)\succ0$, we can solve for $\lambda$, and then derive $\bm y$:
\[
\sum_{i=1}^n\frac{\bar{z_i}^2\mu_i}{(1+\lambda\mu_i)^2}=1
\]
\end{enumerate}
\end{example}
\subsection{Feasible diection method}
First let's discuss the motivation of feasible direction method:
\begin{proposition}
Consider the problem (\ref{Eq:7:2}), $x^*\in X$ is stationary point if and only if
\[
\begin{array}{ll}
x^*=[x^*-\alpha\nabla f(x^*)]^+.
&
\forall\alpha>0
\end{array}
\]
\end{proposition}
\begin{proof}
\begin{itemize}
\item
For the forward direction, the stationarity of $x^*$ is equivalent to 
\begin{equation}\label{Eq:7:5}
\begin{array}{ll}
\inp{\nabla f(x^*)}{(x-x^*)}\ge0,
&
\forall x\in X
\end{array}
\end{equation}
Let $z:=x^*-\alpha\nabla f(x^*)$, and consider
\begin{subequations}\label{Eq:7:6}
\begin{align}
\min_x\|x-z\|^2&=\|x-x^*+\alpha\nabla f(x^*)\|^2\\
&=\|x-x^*\|^2+2\alpha\inp{\nabla f(x^*)}{(x-x^*)}+\alpha^2\|\nabla f(x^*)\|^2\\
&\ge\alpha^2\|\nabla f(x^*)\|^2,
\end{align}
\end{subequations}
which implies the minimum point for (\ref{Eq:7:6}) is $x^*$, i.e., $x^*=[z]^+$.
\item
For the reverse direction, assume $x^*$ is not the stationary point, i.e., $\exists x^0\in X$ such that $\inp{\nabla f(x^*)}{(x^0-x^*)}<0$. We set $d^0=x^0-x^*$. For fixed $\alpha>0$, we construct $x^1\in X$ such that
\[
d^1:=x^1-x^*:=\frac{-\alpha\inp{\nabla f(x^*)}{d^0}}{\|d^0\|_2^2}d^0
\]
Substituting $x^1$ into the problem(\ref{Eq:7:6}), we have
\begin{align*}
\|x^1-z\|^2&=\|d^1\|^2+2\alpha\inp{\nabla f(x^*)}{d^1}+\alpha^2\|\nabla f(x^*)\|^2\\
&=\left[\frac{\alpha^2\inp{\nabla f(x^*)}{d^0}^2}{\|d^0\|^4}\right]\|d^0\|^2
+2\alpha\cdot\frac{-\alpha\inp{\nabla f(x^*)}{d^0}}{\|d^0\|_2^2}\inp{\nabla f(x^*)}{d^0}+\alpha^2\|\nabla f(x^*)\|^2\\
&=-\frac{\alpha^2\inp{\nabla f(x^*)}{d^0}^2}{\|d^0\|^2}+\alpha^2\|\nabla f(x^*)\|^2\\
&<\alpha^2\|\nabla f(x^*)\|^2=\|x^*-z\|^2,
\end{align*}
i.e., $x^*$ cannot be the minimizer of problem(\ref{Eq:7:6}), which contradicts to the fact that $x^*=[z]^+$.
\end{itemize}

\end{proof}
The idea for feasible direction method is that we generate a sequence of $\{x^k\}\subseteq X$ such that $f(x^{r+1})\le f(x^r)$, or equivalently,  
\begin{subequations}
find $\bar{x^r}\in X$ such that the $\bar{x^r}-x^r$ is the descent direction:
\begin{equation}
\inp{\nabla f(x^r)}{(\bar{x^r}-x^r)}\le0
\end{equation}
and therefore
\begin{equation}
\begin{array}{ll}
x^{r+1} = x^r+\alpha_r(\bar{x^r}-x^r),
&
\alpha_r\in(0,1]
\end{array}
\end{equation}
\end{subequations}
The feasibility of $x^{r+1}$ is guaranteed since $x^{r+1}$ is a convex combination of feasible points $x^r$ and $\bar{x^r}$. Here the problem remains to find $\bar{x}^r$

\paragraph{Projection Gradient Method}
One way of finding $\bar{x}^r$ is to compute $x^r-s_r\nabla f(x^r)$ and project it back into $X$, as $\bar{x^r}$:
\[
\begin{array}{ll}
\bar{x^r}=[x^r-s_r\nabla f(x^r)]^+,
&
s_r>0
\end{array}.
\]
\paragraph{Conditional Gradient}
Another way is to linearize the objective function and solve for $\bar{x}^r$:
\[
\bar{x}^r\approx\arg\min_{x\in X}f(x)\implies
\bar{x}^r=\arg\min_{x\in X}f(x^r)+\inp{\nabla f(x^r)}{(x-x^r)}.
\]

