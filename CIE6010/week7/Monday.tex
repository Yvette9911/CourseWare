
\chapter{Week7}

\section{Monday}\index{week7_Tuesday_lecture}
\subsection{Announcement}
\begin{itemize}
\item
The new homework is assigned. You are required to write a code for linear programming. The detail will be announced in tutorial.
\item
The mid-term will be some written problem and computation porblem. We will cover contents in Chapter 1,2,3,4 in the textbook. 
\end{itemize}
\subsection{Recap about linear programming}
The standard form of linear programming is
\[
\begin{array}{ll}
\min&\bm c\trans\bm x\\
\mbox{such that}&\bm{Ax}=\bm b\\
&\bm x\ge0
\end{array}
\]
Define the Lagrange function
\[
\begin{aligned}
L(\bm x,\bm y,\bm z)&=\bm c\trans\bm x - \bm y\trans(\bm{Ax}-\bm b) - \bm z\trans\bm x\\
&=(\bm c-\bm A\trans\bm y-\bm z)\trans\bm x+\bm b\trans\bm y
\end{aligned}
\]
From this Lagrange function we define a dual function
\[
Q(\bm y,\bm z)=\inf_{\bm x}L(\bm x,\bm y,\bm z)=\left\{
\begin{aligned}
\bm b\trans\bm y,&\quad\mbox{if }\bm c-\bm A\trans\bm y-\bm z=0\\
-\infty,&\quad\mbox{otherwise}
\end{aligned}
\right.
\]
For any feasible $\bm x$ and $\bm z\ge0$, we always have
\[
Q(\bm y,\bm x)\le\bm c\trans\bm x-\bm z\trans\bm x\le\bm c\trans\bm x
\]
Moreover, 
\[
\max_{\bm A\trans y+\bm z=\bm c,\bm z\ge0}\bm b\trans\bm y=
\sup_{\bm z\ge0}Q(\bm y,\bm z)\le\min_{\bm{Ax}=\bm b,\bm x\ge0}\bm c\trans\bm x
\]
\begin{itemize}
\item
The weak duality says that the optimal dual objective is always no more than the optimal primal objective.
\item
The strong duality says that the optimal dual objective is equal to the optimal primal objective.
\end{itemize}
\begin{theorem}
The LP optimality condition is given by:
\begin{enumerate}
\item
Primal-Feasibility:
\[
\bm{Ax}=\bm b,\bm x\ge0
\]
\item
Dual-Feasibility:
\[
\bm A\trans\bm y+\bm z=\bm c,\bm z\ge0
\]
\item
Complementarity/Strong-Duality:
\[
\bm x\circ \bm z=0
\]
\end{enumerate}
\end{theorem}
\begin{remark}
Hence, the LP is essentially solving the linear systems. The Assignment 5 aims to solve the linear programming problem via primal-dual algorithm. Furthermore, it is called the primal-dual interior-point method (pdipm). The idea of interior-point method is to set the barrier:
\[
\bm x\circ\bm  z=u*\bm 1,
\]
and with $u\to0$, $\bm x\circ \bm z=0$.
\end{remark}
\paragraph{Something about A5}
During the implementation, you need to solve the system
\[
\underbrace{(A(Z^{-1}X)A\trans)}_{\bm B}dy=rh_s
\]
Given that the $A$ is sparse, we call the command $\textrm{chol}$ to decompose $\bm M=\bm R\trans\bm R$ with $\bm R$ to be upper triangular. Then we obtain the solution 
\[
dy=R/(R'/rhs)
\]
However, it is still time-consuming. We can accelerate it by re-arrange the order of the linear system by the command $\textrm{symamd}$. The detailed handle discussing the advantage for re-ordering is attached in assignment.
\paragraph{Incremental Problem}When solving the least squares problem
\[
\min\frac{1}{2}\sum_{i=1}^m(\bm a_i\trans\bm x-\bm b_i)^2=\frac{1}{2}\|\bm{Ax}-\bm b\|_2^2,
\]
the optimal analytic solution is given by $\bm x^*=(\bm A\trans\bm A)^{-1}\bm A\trans\bm b$. Let's try the numerical method:
\[
\bm x^{k+1}\leftarrow\bm x^k-\alpha^k\bm g^1-\alpha^kg^2-\cdots-\alpha^k\bm g^m,
\]
where $\alpha^k=\frac{\theta}{k}$. Prof.YZ's $\alpha$ is greater than $10$. Although we have the analytic solution, it may not necessarily better than the numerical one since it will perturbed by the noise.

\subsection{Optimization over convex set}
For the optimization problem
\begin{equation}
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&x\in X
\end{array}\label{Eq:7:1}
\end{equation}
with $X$ to be convex, the necessary conditon for optimality is
\[
\nabla\trans f(\bm x^*)(\bm x-\bm x^*)\ge0,\quad\forall x\in X
\]
If $f$ is convex, then it is indeed the necessary and sufficient condition.
\paragraph{Example}
For $X=\{x\mid x\ge0\}$, we find
\[
(\nabla f(\bm x^*))_i=\left\{
\begin{aligned}
\frac{\partial f(\bm x^*)}{\partial x_i}\ge0,&\quad x_i^*=0\\
\frac{\partial f(\bm x^*)}{\partial x_i}=0,&\quad x_i^*>0
\end{aligned}
\right.
\]
\paragraph{Project}
Given $z\in\mathbb{R}^n$, the least square problem
\[
\begin{array}{ll}
\min&\|\bm x-\bm z\|_2\\
&\bm x\in X
\end{array}
\]
is essentially the projection problem:
\[
[\bm z]^+=\bm x^*=\arg\min_{\bm x\in X}\|\bm x-\bm z\|
\]
\begin{proposition}[Non-expansive]
\[
\|[\bm z_1]^+ - [\bm z_2]^+\|\le\|\bm z_1-\bm z_2\|
\]
with $X$ to be a convex set.
\end{proposition}
\paragraph{Projection-based optimality condition}
Applying projection property, we derive alternative optimality condition for (\ref{Eq:7:1}):
\begin{proposition}
$\bm x^*$ is a stationary point iff
\[
\bm x^*=[\bm x^*-\alpha\nabla f(\bm x^*)]+,
\]
for $\forall\alpha>0$
\end{proposition}









