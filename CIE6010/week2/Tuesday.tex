
\chapter{Week2}
\section{Monday}\index{week2_Tuesday_lecture}
\subsection{Reviewing and Announments}
Tutorial: Thursday 7:00pm -9:00pm, ChengDao 208

Homework is due every Monday.

The first homework has been uploaded.

To proof the optimality condition in $\mathbb{R}^n$, we set $h(t) = f(x^*+td)$ for fixed $x^*$ and $d$. It follows that
\[
h'(t)=\nabla\trans f(x^*+td)d
\]
and
\[
h''(t)=d\trans\nabla^2f(x^*+td)d
\]
By the optimality condition for $\mathbb{R}$, we derive the necessary condition:
\[
\left\{
\begin{aligned}
\mbox{$h'(0)=\nabla\trans f(x^*)d=0$ for $\forall d$}\implies \mbox{$\nabla f(x^*)=0$;}\\
\mbox{$h''(0)=d\trans\nabla^2f(x^*)d=0$ for $\forall d$}
\implies
\mbox{$\nabla^2f(x^*)\succeq0$}
\end{aligned}
\right.
\]
together with the sufficient condition:
\[
\left\{
\begin{aligned}
\mbox{$\nabla f(x^*)=0$;}\\
\mbox{$\nabla^2f(x^*)\succ0$}
\end{aligned}
\right.
\]
\subsection{Quadratic Function Case Study}
Given a quadratic function 
\[
f(\bm x) =\frac{1}{2}\bm x\trans\bm Q\bm x+\bm b\trans\bm x
\]
w.l.o.g., assume the matrix $\bm Q$ is symmetric (recall the quadratic section studied in linear algebra).


\begin{definition}[Stationarity]
A point $\bm x^*$ is said to be the stationary point of $f(\bm x)$ if $\nabla f(\bm x^*)=\bm0$.
\end{definition}
To minimize such a function without constraint, we apply the optimality condition:
\begin{enumerate}
\item
The first order optimality condition is given by:
\[
\nabla f(\bm x) = \bm Q\bm x+\bm b=\bm0
\]
The stationary point of the quadratic function $f(\bm x)$ exists iff $\bm b\in\mathcal{C}(\bm Q).$
\item
The second order necessary condition should be:
\[
\nabla^2 f(\bm x)=\bm Q\succeq0
\]
\end{enumerate}
For this special case, if $\bm Q\succeq0$, then $f(\bm x)$ is convex, the solutions to $\nabla f(\bm x) =\bm0$ are local minimum points. Furthermore, they are global minimum points (prove by Taylor Expansion). However, for general functions, we cannot obtain such good results.

\paragraph{Least Squares Problem} Such a problem has been well-studied in statistics given by:
\[
\min_{\bm x}f(\bm x):=\frac{1}{2}\|\bm{Ax}-\bm b\|_2^2
\]
The first order derivative of the minimizer should satisfy:
\[
\nabla f(\bm x)=\bm A\trans(\bm A\bm x-\bm b)
\]
Note that $\bm A\trans\bm b\in\mathcal{C}(\bm A\trans\bm A)$, thus the least squares problem always has a solution. However, such a solution is not unique unless $\bm A$ is full rank.
\paragraph{A Non-trival Quadratic Function}
To minimize the function
\[
f(x,y)=\frac{1}{2}(\alpha x^2+\beta y^2)-x
\] 
We take the first order derivative to be zero:
\[
\nabla f(x,y)=\begin{bmatrix}
\alpha x-1\\\beta y
\end{bmatrix}=\bm0
\]
The second order derivative is given by:
\[
\nabla^2 f(x,y)=\begin{bmatrix}
\alpha&0\\0&\beta
\end{bmatrix}
\]
The optimal solutions depend on the value of $\alpha$ and $\beta$: (although we haven't introduce the definition for convex formally)
\begin{itemize}
\item
If $\alpha,\beta>0$, then this problem is \emph{strongly convex}. By the necessary and sufficient optimality condition for convex problem, we find that $(\frac{1}{\alpha},0)$ is the unique local minimum (It is also the global minimum by plotting the figure).
\item
If $\alpha=0$, this problem has no solution. The objective value $f(x,y)\to-\infty$ as $x\to
\infty$.
\item
If $\beta=0,\alpha>0$, this problem is convex.  By the necessary and sufficient optimality condition for convex problem, $\{(\frac{1}{\alpha},\xi)\mid \xi\in\mathbb{R}\}$ is the set of local minimum. (By plotting the graph, we find that such set is the set of global minimum points)
\item
For $\alpha>0,\beta<0$ case, this problem is non-convex. Actually, $f(x,y)\to-\infty$ as $y\to
\infty$. Hence, this problem has no global minimum point.
\end{itemize}

\paragraph{A Non-trival Function Study}
To minimize the function
\[
\begin{array}{ll}
\min&f(\bm y)=e^{y_1}+\cdots+e^{y_n}\\
\mbox{such that}&y_1+\cdots+y_n=S
\end{array}
\]
We can transform such a constrainted optimization problem into unconstrainted. Let $y_n=S-y_1-\cdots-y_{n-1}$ and substitute it into the objective function, it suffices to solve
\[
\min e^{y_1}+\cdots+e^{y_{n-1}}+e^{S-y_1-\cdots-y_{n-1}}
\]
The stationary point should satisfy:
\[
e^{y_i}=e^{S-y_1-\cdots-y_{n-1}},
\qquad
i=1,2,\dots,n-1
\]
Or equivalently, $y_1=y_2=\cdots=y_{n-1}=y_n$. Hence we derive the unique stationary point:
\[
y_1^*=y_2^*=\cdots=y_n^*=\frac{S}{n}
\]
The value on the stationary point is $f(y^*)=ne^{S/n}$. By checking the second order sufficient optimality condition,
\[
\frac{f}{\partial y_i\partial y_j}=\left\{
\begin{aligned}
e^{y_i}+e^{S-y_1-\cdots-y_{n-1}}&\quad i=j\\
e^{s-y_1-\cdots-y_{n-1}}&\quad i\ne j
\end{aligned}
\right.\implies
\nabla^2 f=e^{s-y_1-\cdots-y_{i-1}-y_i-y_{n-1}}\bm E
+\diag(e^{y_1},\dots,e^{y_{n-1}})
\]
where $\bm E$ is a matrix with entries all ones. Thus $\nabla^2 f\succ0$ for any stationary point. By the second order sufficient optimality condition, this stationary point is local minimum. Actually, for this special problem, this unique local minimum point is the global minimum.
\begin{remark}
In this problem, we find that this stationary point is the unique local minimum point, but the unique local minimum point is not necessarily the global minimum point, unless the function is \emph{coercive} or the feasible region is compact. Here is the counter-example: $f(x)=x^2-x^4$. We will discuss the definition for coercive in the future.
\end{remark}









