
\chapter{Week6}

\section{Monday}\index{week6_Tuesday_lecture}
\subsection{Announcement}
The new homework is assgined, which is due on Friday. The programming task is relatively easy this week, but requires research on some paper.
\subsection{Introduction to Quasi-Newton Method}
\[
x^{k+1} \leftarrow x^k -\alpha_k D_k\nabla f(x^k)
\]

Here $D^k$ is the approximation of the inverse of the Hessian matrix. Our initial guess is usually $D_0=\tau I$, and then
\begin{equation}\label{Eq:6:1}
D^{k+1} \leftarrow \mbox{Update}(D^k,s^k,y^k),
\end{equation}
with $s^k = x^{k+1} -  x^k$, $y^k = \nabla f(x^{k+1}) - \nabla f(x^k)$. 
\paragraph{QN-Equation} Note that for any $x,x'$ in the domain,
\[
\nabla f(x') - \nabla f(x)  = \left[\int_0^1\nabla^2f(x+\tau(x'-x))\diff\tau\right](x'-x)\approx
\nabla^2f(x)(x'-x)
\]
Hence, we assume that the update rule (\ref{Eq:6:1}) has to satisfy the Quasi-Newton equation:
\[
D^{k+1}s^k = y^k
\]

\paragraph{Least-Change}
Moreover, we want to use previous information, i.e., 
\[\begin{array}{ll}
\min &\|D^{k}-D^k\|_{w}\\
\mbox{such that}&Ds^k=y^k\\
&D\succ0
\end{array}
\]
Prof.YZ did his phd research on this topic.

The convergence rate of QN-equation could be super-linear. 
\subsection{Constrainted Optimization Problem}
Given the un-reduency problem
\begin{equation}\label{Eq:6:1}
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&Ax=b,\quad A\in\mathbb{R}^{m\times n},m<n
\end{array}
\end{equation}
we can convert this problem into unconstrainted. Let $x=A\trans u+B\trans v$ with:
\[
\begin{array}{ll}
AB\trans=0;\qquad\qquad
&
\mbox{$\begin{bmatrix}
A\\B
\end{bmatrix}_{m+(n-m)}$ is non-singular}
\end{array}
\]
It follows that
\[
AA\trans u=b\implies u^*=(AA\trans)^{-1}b,
\]
and therefore every $x=A\trans u^*+B\trans v$ satisfies the constraint. It suffices to solve
\[
\min_{v}f(A\trans u^*+B\trans v):=h(v)
\]
Thus the problem becomes unconstraint.
\paragraph{Optimality Condition for (\ref{Eq:6:1})}
 The necessary optimality condition is 
\[\nabla h(v)=
B\nabla f(x)=0,\quad\mbox{for }Ax=b.
\]
which basically says that $\nabla f(x)\in\mathcal{R}(A\trans)$ for $x$ such that $Ax=b$, i.e.,
\[\left\{
\begin{aligned}
\nabla f(x)&=A\trans\lambda\\
Ax&=b
\end{aligned}
\right.\qquad
\mbox{1st order necessary condition for $(\ref{Eq:6:1})$}
\]
Therefore, optimization problem $(\ref{Eq:6:1})$ is essentially a linear equality constrainted optimization.
\paragraph{Optimality condition for general function}
For a more general problem 
\begin{equation}\label{Eq:6:2}
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&C(x)=0
\end{array}
\end{equation}
The necessary optimality condition is
\[\left\{
\begin{aligned}
\nabla f(x)&=\nabla C(x)\lambda\\
C(x)&=0
\end{aligned}
\right.
\]
Beside from the necessary optimality condition, the problem should meet the Constraint Qualification (CQ), i.e., gradient should be linear independent. We will discuss this issue in the future.

\subsection{Announcement on Assignment}
\begin{enumerate}
\item
For phd student, please do this problem: Exercise 2.2.1
\item
In the programming project, we are required to sovle the problem
\[
\min_{X\in\mathbb{R}^{n\times k}}\|A - XX\trans\|_F^2
\]
The Gauss-Newton method to this problem is even faster than SVD or something else.
\item
The second programming task is about Linear Programming, please go to tutorial to have idea about that:
\begin{equation}\label{Eq:6:3}
\begin{array}{ll}
\min&c\trans x\\
\mbox{such that}&Ax=b, A\in\mathbb{R}^{m\times n},m<n\\
&x\ge0
\end{array}
\end{equation}
We can approximate this problem by 

\[
\begin{array}{ll}
\min&c\trans x \underbrace{- \mu\sum\ln x_i}_{\text{barrier}}:=f(x)\\
\mbox{such that}&Ax=b, A\in\mathbb{R}^{m\times n},m<n
\end{array}
\]
with $\mu>0$. As $\mu\to0$, this problem is the approximation of (\ref{Eq:6:3}). As long as it satisfies the necessary optimality condition, we get the minimum point:
\[
\left\{
\begin{aligned}
c-\mu\begin{pmatrix}
1/x_1\\\vdots\\1/x_n
\end{pmatrix}&=A\trans y\\
Ax&=b
\end{aligned}\right.
\]
Set $z = \mu./x$, it suffices to solve
\[
\left\{
\begin{aligned}
c-z&=A\trans y\\
Ax&=b\\
x_iz_i&=u,\quad i=1,2,\dots,n
\end{aligned}
\right.
\Longleftrightarrow
F_{\mu}(x,y,z)=\begin{bmatrix}
A\trans y+z-c\\
Ax-b\\
x\circ z-u
\end{bmatrix}=\begin{pmatrix}
0\\0\\0
\end{pmatrix}
\]


\end{enumerate}
\paragraph{Optimal Algorithm for optimization}
Given a collection of problem, the optimality of algorithm is defined as the performance of such for the worse case.

The gradient descent method gives
\begin{align*}
x^{k+1}&=x^0-\alpha_0g_0-\alpha_1g_1-\cdots-\alpha_kg_k\\
&=x^0-\Span\{g_0,g_1,\dots,g_n\}
\end{align*}
The optimal accelerated method's performance order is $O(\frac{1}{k^2})$.

\subsection{Introduction to Stochastic optimization}
Consider the Stochastic optimization problem
\[
\min_x\mathbb{E}_{\xi}f(x,\xi)
\]
When the distribution of $\xi$ is unknown, we take samples and then solve
\[
\min_x\frac{1}{m}\sum_{i=1}^mf(x,\xi_i)\Longleftrightarrow
\min\sum_{i=1}^mf_i(x,\xi)
\]
If $f_i(x):=\|g_i(x) - y_\xi\|^2$, the origin problem becomes the sum of least squares.

We apply gradient descent method to solve this problem:
\[
\nabla \frac{1}{2}f(x)=\sum_{i=1}^m\nabla g_i(x)\left[g_i(x) - y_i\right]
\]
We will explain this application in detail in the future.



\section{Tutorial: Monday}
This tutorial aims to cover topics in Assignment 4 and previous assignments.

\subsection{LP Problem}
\[
\begin{array}{ll}
\min&c\trans x\\
\mbox{such that}&Ax=b\\
&x\ge0
\end{array}
\]
It suffices to solve the nonlinear system:
\[
F(x,y,z)=\begin{pmatrix}
A\trans y+z-c\\Ax-b\\x\circ z-\mu
\end{pmatrix}=0,\quad x,z\ge0
\]
Or equivalently,
\[
\left\{
\begin{aligned}
F'(x,y,z)\begin{pmatrix}
dx\\dy\\dz
\end{pmatrix}&=\begin{pmatrix}
c-A\trans y-z\\
b-Ax\\
\mu-x\circ z
\end{pmatrix}\\
F'&=\begin{pmatrix}
A&0&0\\0&A\trans&I\\z&0&x
\end{pmatrix}\\
z&:=\diag(z_1,\dots,z_n)\\
x&:=\diag(x_1,\dots,x_n)
\end{aligned}\right.
\]
We solve this system by Gaussian Elimination:
\[
\begin{pmatrix}
A\trans&I&0\\0&0&A\\0&X&Z
\end{pmatrix}\begin{pmatrix}
dy\\dz\\dz
\end{pmatrix}=\begin{pmatrix}
r_d\\r_p\\r_c
\end{pmatrix}\implies
\left[
\begin{array}{@{}ccc|c@{}}
A\frac{X}{Z} A\trans&0&0&A\frac{Xr_d-r_c}{Z}+r_p\\
0&0&A&r_p\\
0&X&Z&r_c
\end{array} \right]
\]
and therefore
\[
\begin{pmatrix}
A\frac{X}{Z} A\trans&0&0\\0&0&A\\0&X&Z
\end{pmatrix}\begin{pmatrix}
dy\\dz\\dx
\end{pmatrix}=\begin{pmatrix}
A\frac{Xr_d-r_c}{Z}+r_p\\r_p\\r_c
\end{pmatrix}
\]
Hint: Use $spdiags(X./Z)$ to accelerate computation.

\subsection{Gauss-Newton Method}
In second task, we are required to minimize $\|XX\trans-A\|^2$, during calculation, we need to arrange wisely to determine which $X,Y,A,Z$ to compute first.
\subsection{Introduction to KKT and CQ}
Consider the problem
\[
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&g(x)\le0\\
&h(x)=0
\end{array}
\]
The KKT is essentially first-order necessary conditions for it.
\[
\begin{array}{ll}
\mbox{Dual Feasibility}&\nabla_x(f(x)+\mu g(x)+\gamma h(x))=0,\quad \mu\ge0\\
\mbox{Primal Feasibility}&g(x)\le0\\
&h(x)=0\\
&z\cdot g(x)=0,\quad z\ge0
\end{array}
\]
However, the KKT condition does not hold for optimal solution necessarily. Furthermore, it must meet the Constraint Qualification.

\begin{example}
\[
\begin{array}{ll}
\min&x\\
\mbox{such that}&x^2\le0
\end{array}
\]
If we use KKT condition, then $\nabla_x(x+\lambda x^2)=1+2\lambda x=1$ at $x=0$. This contradction is because we haven't tested the Constraint Qualification.
\end{example}
There are two kinds of CQ, either one is satisfied is OK.
\begin{enumerate}
\item
Slaters Condition: for convex problem, if exists $x$ such that $g(x)<0$ and $h(x)=0$.
\item
Linear Independence Constraint Qualification:
\[
\nabla g_x,\qquad \nabla h
\]
are linear independent.
\end{enumerate}

















