
\section{Wednesday}\index{week7_Thursday_lecture}
\subsection{Trust Region problem}
Given the unconstraint problem $\min f(\bm x)$, at $\bm x^k$,
\[
\begin{array}{ll}
\min&m_k(\bm p)=f(\bm x^k)+\nabla\trans f(\bm x^k)\bm p+\frac{1}{2}\bm p\trans\bm B_k\bm p\\
\mbox{such that}&\|\bm p\|\le\Delta
\end{array}
\]
In our project we need to slove it very accurately.
\paragraph{Trust Region sub-problem}
\[
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm g\trans\bm p\\
\mbox{such that}&\|\bm p\|\le\Delta
\end{array}
\]
Necessary condition: $\bm p^*$ is a local minimum implies that (KKT condition)
\begin{subequations}
\begin{align}
(\bm B+\lambda\bm I)\bm p^*&=-\bm g\label{Eq:12:8:a}\\
\lambda&\ge0\\
\lambda(\Delta - \|\bm p\|)&=0\\
\|\bm p\|&\le\Delta
\end{align}
\end{subequations}
The second order necessary condition is
\[
\bm v\trans(\bm B+\lambda\bm I)\bm v\ge0,\forall \bm v\perp\bm p^*
\]
In addition, we may replace the 2nd order conditon with a little bit stronger condition. 
\begin{proposition}
$\bm B+\lambda\bm I\succeq0$ together with KKT condition are sufficient condition for global minimum point.
\end{proposition}
\begin{proof}
For the forward direction, (\ref{Eq:12:8:a}) implies that $p^*$ is a global minimum for the function
\[
\hat m(\bm p)=m(\bm p)+\frac{\lambda}{2}\bm p\trans\bm p,
\]
by checking the first and second derivative of $p^*$, which implies that $\hat m(\bm p^*)\le \hat m(\bm p)$. Moreover, this relation can be written as:
\begin{align*}
m(\bm p^*)&\le m(\bm p)+\frac{\lambda}{2}(\bm p\trans\bm p-(\bm p^*)\trans(\bm p^*))\\
&=m(\bm p)+\frac{\lambda}{2}(\bm p\trans\bm p-\Delta^2+\Delta^2-(\bm p^*)\trans(\bm p^*))\\
&=m(\bm p)+\frac{\lambda}{2}(\bm p\trans\bm p-\Delta^2)\\
&=m(\bm p),\quad\forall \mbox{feasible }\bm p.
\end{align*}

For the reverse direction, assume $\bm p^*$ is global minimum. If $\|\bm p^*\|<\Delta$, then $\lambda=0$. In this case, $p^*$ is the global minimum for $m(\bm p)$ without constraint, then the quadratic $m(\bm p)$ must be convex, which implies $\bm B\succeq0$.

If $\|\bm p^*\|=\Delta$, it suffices to show 
\[
\frac{1}{2}(\bm p-\bm p^*)\trans(\bm B+\lambda\bm I)(\bm p-\bm p^*)\ge0,
\]
for $\bm p,\bm p^*$ on the ball $\{\bm p\mid\|\bm p\|=\Delta\}$. Check that
\begin{align*}
\frac{1}{2}(\bm p-\bm p^*)\trans(\bm B+\lambda\bm I)(\bm p-\bm p^*)&=\frac{1}{2}\bm p\trans(\bm B+\lambda\bm I)\bm p+\frac{1}{2}(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p-\bm p\trans(\bm B+\lambda\bm I)\bm p^*\\
&=\frac{1}{2}\bm p\trans(\bm B+\lambda\bm I)\bm p+\frac{1}{2}(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p
+
\bm p\trans\bm g\\
&=\frac{1}{2}\bm p\trans(\bm B+\lambda\bm I)\bm p+\frac{1}{2}(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p
+
\bm p\trans\bm g\\
&\qquad-(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p
-
-(\bm p^*)\trans\bm g\\
&=\hat m(\bm p) - \hat m(\bm p^*)
\\
&=m(\bm p) - m(\bm p^*) + \frac{\lambda}{2}(\bm p\trans\bm p-(\bm p^*)\trans\bm p^*)\\
&= m(\bm p) - m(\bm p^*)\ge0,
\end{align*}
which implies that $\bm B+\lambda\bm I\succeq0$.
\end{proof}
\paragraph{Barely Pass Suggestion}
You may use the command $fmincon$ to implement, but you can barely pass.

\section{Monday Tutorial: Trust Region Sub-problem}
\subsection{ADMM}
This project aims to minimize the function
\begin{equation}
\begin{array}{ll}
\min&\frac{1}{2}\bm x\trans\bm A\bm x-\bm b\trans\bm x+\frac{1}{2}\bm y\trans\bm A\bm y-\bm c\trans\bm y\\
&\frac{1}{2}(\bm x\trans\bm x-1)=0\\
&\frac{1}{2}(\bm y\trans\bm y-1)=0\\
&\bm x\trans\bm y=0
\end{array}
\end{equation}
We only penalize the third constraint, and therefore our augmented Lagrangian function is given by:
\[
L(\bm x,\bm y,\bm\lambda)=q_b(\bm x)+q_c(\bm y)+\lambda_1(\frac{\bm x\trans\bm x-1}{2})+\lambda_2(\frac{\bm y\trans\bm y-1}{2})+\lambda_3(\bm x\trans\bm y)+\frac{\rho}{2}(\bm x\trans\bm y)^2
\]
with $\bm x\trans\bm x=1$ and $\bm y\trans\bm y=1$.

Thus it suffices to solve the subproblem
\begin{align*}
\bm x^{k+1}&=\arg\min_{\bm x}\frac{1}{2}\bm x\trans\left(
\bm A+\lambda_1\bm I+\rho(\bm y^k)\trans(\bm y^k)
\right)\bm x+(\lambda_3\bm y^k-\bm b)\trans\bm y\\
\bm y^{k+1}&=\arg\min_{\bm y}\frac{1}{2}\bm y\trans\left(
\bm A+\lambda_2\bm I+\rho(\bm x^k)\trans(\bm x^k)
\right)\bm y+(\lambda_3\bm x^k-\bm c)\trans\bm y\\
\lambda_3&=\lambda_3+\rho(\bm x^{k+1})\trans\bm y^{k+1}
\end{align*}
\subsection{Trust Region Subproblem}
The TRS essentially aims to solve
\[
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm y\trans\bm p\\
&\|\bm p\|\le1
\end{array}
\]
\begin{itemize}
\item
Call the command $fmincon$, which applies the primal-dual or sequential quadratic programming method.
\item
Consider the optimality condition. This kind of method is super-difficult.
\end{itemize}

There are many other methods to solve the TRS:
\paragraph{Dog-leg}



\paragraph{Truncated Conjugate Gradient}





















