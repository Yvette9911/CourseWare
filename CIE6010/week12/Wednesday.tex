
\section{Wednesday}\index{week7_Thursday_lecture}
Given the unconstraint problem $\min f(\bm x)$, at $\bm x^k$,
\[
\begin{array}{ll}
\min&m_k(\bm p)=f(\bm x^k)+\nabla\trans f(\bm x^k)\bm p+\frac{1}{2}\bm p\trans\bm B_k\bm p\\
\mbox{such that}&\|\bm p\|\le\Delta
\end{array}
\]
In our project we need to slove it very accurately.
\paragraph{Trust Region sub-problem}
\[
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm g\trans\bm p\\
\mbox{such that}&\|\bm p\|\le\Delta
\end{array}
\]
Necessary condition: $\bm p^*$ is a local minimum implies that (KKT condition)
\begin{subequations}
\begin{align}
(\bm B+\lambda\bm I)\bm p^*&=-\bm g\label{Eq:12:8:a}\\
\lambda&\ge0\\
\lambda(\Delta - \|\bm p\|)&=0\\
\|\bm p\|\le\Delta
\end{align}
\end{subequations}
The second order necessary condition is
\[
\bm v\trans(\bm B+\lambda\bm I)\bm v\ge0,\forall \bm v\perp\bm p^*
\]
In addition, we replace the 2nd order conditon with a little bit stronger condition. 
\begin{proposition}
$\bm B+\lambda\bm I\succeq0$ together with KKT condition are iff for $p^*$ to be a global minimum.
\end{proposition}
\begin{proof}
Assume all the condition holds. Then (\ref{Eq:12:8:a}) implies that $p^*$ is a global minimum of
\[
\hat m(\bm p)=m(\bm p)+\frac{\lambda}{2}\bm p\trans\bm p
\]
since
\[
\nabla^2\hat m(\bm p)=\bm B+\lambda\bm I\succeq0
\]
and
\[
\nabla \hat m(\bm p)=(\bm B+\lambda\bm I)p^*+\bm g=0
\]
which implies that $\hat m(\bm p^*)\le \hat m(\bm p)$, i.e.,
\begin{align*}
m(\bm p^*)&\le m(\bm p)+\frac{\lambda}{2}(\bm p\trans\bm p-(\bm p^*)\trans(\bm p^*))\\
&=m(\bm p)+\frac{\lambda}{2}(\bm p\trans\bm p-\Delta^2+\Delta^2-(\bm p^*)\trans(\bm p^*))\\
&=m(\bm p)+\frac{\lambda}{2}(\bm p\trans\bm p-\Delta^2)\\
&=m(\bm p),\quad\forall \mbox{feasible }\bm p.
\end{align*}

For the reverse direction, assume $\bm p^*$ is global minimum. If $\|\bm p^*\|<\Delta$, then $\lambda=0$. In this case, $p^*$ is the global minimum for $m(\bm p)$ without constraint, then the quadratic $m(\bm p)$ must be convex, which implies $\bm B\succeq0$.

If $\|\bm p^*\|=\Delta$, it suffices to show 
\[
\frac{1}{2}(\bm p-\bm p^*)\trans(\bm B+\lambda\bm I)(\bm p-\bm p^*)\ge0,
\]
for $\bm p,\bm p^*$ on the ball. Check that
\begin{align*}
\frac{1}{2}(\bm p-\bm p^*)\trans(\bm B+\lambda\bm I)(\bm p-\bm p^*)&=\frac{1}{2}\bm p\trans(\bm B+\lambda\bm I)\bm p+\frac{1}{2}(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p-\bm p\trans(\bm B+\lambda\bm I)\bm p^*\\
&=\frac{1}{2}\bm p\trans(\bm B+\lambda\bm I)\bm p+\frac{1}{2}(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p
+
\bm p\trans\bm g\\
&=\frac{1}{2}\bm p\trans(\bm B+\lambda\bm I)\bm p+\frac{1}{2}(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p
+
\bm p\trans\bm g\\
&\qquad-(\bm p^*)\trans(\bm B+\lambda\bm I)\bm p
-
-(\bm p^*)\trans\bm g\\
&=\hat m(\bm p) - \hat m(\bm p^*)
\\
&=m(\bm p) - m(\bm p^*) + \frac{\lambda}{2}(\bm p\trans\bm p-(\bm p^*)\trans\bm p^*)\\
&= m(\bm p) - m(\bm p^*)\ge0,
\end{align*}
which implies that $\bm B+\lambda\bm I\succeq0$.
\end{proof}
Read chapter 4 for numerical optimization.

You may use the command $fmincon$ to implement, but you can barely pass.

\subsection{Gradient Projection}
\[
\begin{array}{ll}
\min&f(x)\\
\mbox{such that}&x\in X
\end{array}
\]
We have
\[
\bm x^*=\mbox{Proj}_X(\bm x^*-\alpha\nabla f(\bm x^*)),\qquad\forall\alpha>0
\]
Thus we have the iteration
\[
x^{r+1} = \mbox{Proj}_X(x^r - \alpha_r\nabla f(x^r))
\]


At $x^r$, do the proximal problem:
\[
x^{r+1} = \arg\min_{x\in X}f(x)+\frac{1}{2c^k}\|x-x^k\|^2
\]









