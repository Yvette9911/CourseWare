
\chapter{Week12}

\section{Monday}\index{week6_Tuesday_lecture}
\subsection{Comments on Final Project}
\begin{equation}\label{Eq:12:1}
\begin{array}{ll}
\min&f(x,y)=\frac{1}{2}(\bm x\trans\bm A\bm x+\bm y\trans\bm A\bm y) - \bm b\trans\bm x-\bm c\trans\bm y:=q_b(\bm x)+q_c(\bm y)\\
\mbox{such that}&\frac{1}{2}(\bm x\trans\bm x-1)=0\\
&\frac{1}{2}(\bm y\trans\bm y-1)=0\\
&\bm x\trans\bm y=0
\end{array}
\end{equation}

The Lagrangian function is given by:
\begin{equation}
L(\bm x,\bm y,\lambda)=q_b(\bm x)+q_c(\bm y)+\frac{\lambda_1}{2}(\bm x\trans\bm x-1)
+
\frac{\lambda_2}{2}(\bm y\trans\bm y-1)
+
\lambda_3\bm x\trans\bm y
\end{equation}
Stationarity:
\begin{align*}
\bm{Ax}-\bm b+\lambda_1\bm x+\lambda_3\bm y&=0\\
\bm{Ay}-\bm c+\lambda_2\bm y+\lambda_3\bm x&=0\\
\|h(\bm x)\|^2&=0
\end{align*}
The stopping criteria is given by:
\[
\max\left\{
\frac{\|\nabla_{\bm x}L\|}{\|\bm b\|+1},
\frac{\|\nabla_{\bm y}L\|}{\|\bm c\|+1},
\|h(\bm x)\|
\right\}\le\mbox{tol}
\]

The problem(\ref{Eq:12:1}) admits its global minimum since the constraint set is compact. Three methods are suggests: ADMM (yz used this); ALMM; quadratic penalty method.

Sometimes we have the second-kind Lagrangian function
\[
\hat L(\bm x,\bm y,\lambda) = q_b(\bm x)+q_c(\bm y)+\lambda_3\bm x\trans\bm y+\frac{h}{2}(\bm x\trans\bm y)^2
\]
We do the minimization
\begin{equation}
\begin{array}{ll}
\min&\hat L(\bm x,\bm y,\lambda)\\
&\bm x\in X=\{\bm x\mid \bm x\trans\bm x=1\}\\
&\bm y\in Y=\{\bm y\mid \bm y\trans\bm y=1\}\\
\end{array}
\end{equation}

The update rule is therefore given by ($\tau=1.618$)
\[
\lambda_3=\lambda_3+\tau\rho(\bm x\trans\bm y)
\]

Develop a solver for the \emph{trust region} sub-problem
\begin{equation}\label{Eq:12:4}
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm g\trans\bm p\\
&
\bm p\trans\bm p=\Delta^*
\end{array}
\end{equation}
How to get the \emph{global minimum} for the non-convex problem (\ref{Eq:12:4})?


\subsection{Trust Region Method}
Our $\bm B$ has the form
\[
\bm A+\lambda_1\bm I+\rho\bm y\bm y\trans
\]
with sparse $\bm A$ and dense rank 1 matrix $\bm y$. Apply the Sherman-Morrison Formula; apply congugate gradient (command: pcg)

\paragraph{Unconstraint method}
Our goal is to minimize $\min_{\bm x\in\mathbb{R}^n}f(\bm x)$. To choose the step-size, we apply the trust region method. First approximate $f(x)$ with quadratic problem:
\begin{align*}
f(x^*+p)&\approx
f(x^*)+\nabla\trans f(x^*)p+\frac{1}{2}p\trans\nabla^2f(x^*)p\\
&\approx
f(x^*)+\nabla\trans f(x^*)p+\frac{1}{2}p\trans\underbrace{\bm B}_{\text{Approximate Hessian}}p:=m(p)
\end{align*}
It suffices to choose the step size $p$ to minimize the quadratic function above. The constraint is that the $\|p\|$ should be small enough. Thus it suffices to solve
\begin{equation}
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm g\trans\bm p\\
&\|\bm p\|\le\Delta
\end{array}
\end{equation}


Given $\Delta$,
\begin{enumerate}
\item
Solve the TR subproblem to get $p$
\item
$\rho:=\frac{f(x) - f(x+p)}{m(0) - m(p)}$. 
\begin{enumerate}
\item
If $\rho<\frac{1}{4}$, decrease the trust region $\Delta$ to $\frac{1}{4}\Delta$.
\item
If $\rho>\frac{3}{4}$, increase $\Delta$ into $2\Delta$
\item
Else, keep $\Delta$.
\end{enumerate}
\item
\begin{enumerate}
\item
If $\rho\ge\frac{1}{4}$ and $\rho>\eta$, then $x\leftarrow x+p$
\item
Else, $x$ keeps unchanged.
\end{enumerate}
\end{enumerate}




\section{Monday Tutorial}
For the optimization problem
\begin{equation}
\begin{array}{ll}
\min&-0.1(x_1-4)^2+x_2^2\\
&1-x_1^2-x_2^2\le0
\end{array}
\end{equation}
For the MATLAB plot, you may use the command contour or surf or surfc to plot the contour; you may also use the meshgrid. 

For deviation part, first write down the KKT conditions:
\[
L(x_1,x_2,\mu)=-0.1(x_1-4)^2+x_2^2+\mu(1-x_1^2-x_2^2)
\]
Compute the gradient 
\[
\begin{array}{lll}
\nabla_{x_1}L=\nabla_{x_2}L=0,
&
\mu(1-x_1^2-x_2^2)=0,
&
\mu\ge0
\end{array}
\]

Apply the second-order sufficiency for the constraint optimization. 
\[
V(x^*)=\{z: \nabla\trans g_i(x^*)z=0, i\in A(x^*)\}
\]

For the maximization problem
\begin{equation}
\begin{array}{ll}
\max&y\trans x\\
&x\trans Qx\le1
\end{array}
\end{equation}
Compute
\begin{align*}
L(x,y,\mu)&=y\trans x+\mu(1-x\trans Qx)\\
\nabla_xL&=y-\mu Qx=0\\
\mu(1-x\trans Qx)&=0
\end{align*}
Discuss for the $\mu=0$ and $\mu\ne0$ case.

For the second part, method 1 is to apply Cauchy-Schwarz inequality; method 2 is to define
\[
p^*
\]


\subsection{Sub-gradient}
It's defined for convex functions:
\begin{definition}[Sub-gradient]
The sub-gradient for a function $f$ is given by:
\[
\partial f(x)=\{g\mid g\trans (y-x)\le f(y) - f(x),\forall y\}
\]
\end{definition}
\begin{proposition}
$x^*$ minimizes $f(x)$ globally iff $0\in\partial f(x)$
\end{proposition}
\begin{proof}
$
f(y)\ge f(x^*)+0\trans(y-x^*)
$
\end{proof}
\begin{remark}
$-\partial f(x)$ is not necessarily a descent direction; while $-\nabla f(x)$ is a descent direction. For example, $f(x,y)=|x|+2|y|$, and $g=(1,2)\in\partial f(1,0)$, but $(-1,-2)$ is not descent direction.
\end{remark}

\begin{example}
For $f(x) = \|x\|_2^2$, its subgradient is
\[
\partial f(x)=\left\{
\begin{aligned}
\frac{x}{\|x\|_2},&\quad x\ne0\\
\{v:\|v\|_2\le 1\},&\quad x=0
\end{aligned}
\right.
\]
For $f(x)=\|x\|_1$, its subgradient is $\partial f(x)=J_1\times\cdots\times J_n$, with
\[
J_i=\left\{
\begin{aligned}
1,&x_i>0\\
-1,&x_i<0\\
[-1,1],&x_i=0
\end{aligned}
\right.
\]
\end{example}

Usually we apply the proximal operator.
\[
\mbox{prox}_f(x)=\arg\min_{u}f(u)+\frac{1}{2}\|u-x\|^2_2
\]
When $f=\|\cdot\|_1$, it suffices to solve
\[
\arg\min_{u}\|u\|_1+\frac{1}{2}\|u-x\|_2^2
\]

Usually the proximal solution has its closed form. When $f=\|\cdot\|_1$, the solution is soft threshold.

Given the problem
\[
\min\frac{1}{2}\|Ax-b\|_2^2+\lambda\|x\|_1
\]
\begin{enumerate}
\item
Split the problem into
\[
\begin{array}{ll}
\min&\frac{1}{2}\|Ax-b\|_2^2+\lambda\|z\|_1\\
&x=z
\end{array}
\]
\item
The second step is to write down the augmented Lagrangian.
\item
Solve
\begin{align*}
x^{k+1}&=\arg\min_{x}L_\rho(x,y^k,z^k)\\
&=(A\trans A+\rho I)^{-1}(A\trans b + \rho z^k - y^k)\\
z^{k+1}&=\arg\min\lambda\|z\|_1+\frac{\rho}{2}\|x^{k+1}+\frac{y^k}{\rho} - z\|_2^2\\
&=\mbox{prox}_{\lambda/\rho \|\cdot\|}(x^{k+1} + y^k/\rho)\\
&=S_{\lambda/\rho}(\underbrace{x^{k+1} + y^k/\rho}_{u})=\mbox{sgn}(u)\cdot(|u| - \lambda/\rho)_+\\
y^{k+1} &= y^k+\rho(x^{k+1} - z^{k+1})
\end{align*}



\end{enumerate}â€¢










