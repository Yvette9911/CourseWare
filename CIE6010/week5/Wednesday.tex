
\section{Wednesday}\index{week5_Thursday_lecture}
\paragraph{Announcement}
You need to study the textbook by yourself. Those materials will be tested in the mid-term.
\subsection{Comments about Newton's Method}
\paragraph{Newton's Method may not necessarily have quadratic rate of convergence} One counter-example is the optimization on the function $x^2$, where we have the iteration
\[
x^{k+1}=x^k-\frac{1}{2x^k}(x^k)^2=\frac{x^k}{2},
\]
which has linear rate of convergence.
\begin{theorem}[Sufficient Condition for quadratic convergence of Newton's method]
To minimize the function $f(x)$ with gradient function $F(x)=\nabla f(x)$, the Newton's method iteration gives
\[
x^{k+1}=x^k-\left[F'(x^k)\right]^{-1}F(x^k),
\]
the quadratic (local) rate of convergence is guaranteed if the following conditions hold:
\begin{enumerate}
\item
there exists $x^*$ such that $F(x^*)=0$
\item
$\left[F'(x^*)\right]^{-1}$ exists
\item
$F$ is \emph{Lipschitz continuous} near $x^*$.
\end{enumerate}
\end{theorem}
For example, to minimize the function $x^2-x$, the iteration gives the quadratic convergence:
\[
x^{k+1} - 1=\frac{1}{2}(x^k-1)(1-\frac{1}{x^k})=
O((x^k-1)^2)
x^{k+1}=x^k-(2x^k)
\]

\subsection{Constant Step-Size Analysis}
For the unconstraint minimization
\[
\min_{x\in\mathbb{R}^n}f(x),
\]
with convex function $f\in\mathcal{C}^1$, our iteration for constant step-size is given by
\[
x^{k+1} = x^k-\frac{1}{L}\nabla f(x^k).
\]

Now we are interested in the convergence rate of this choice of step-size.
\begin{theorem}[Convergence Rate for invariant step-size]
Given a convex function $f\in\mathcal{C}^2$ with Lipschitz gradient, i.e.,
\[
\|\nabla f(x)-\nabla f(y)\|\le L\|x-y\|^2,
\]
the iteration $x^{k+1} = x^k-\frac{1}{L}\nabla f(x^k)$ with local minimum point $x^*$ gives \mbox{super-linear} convergence rate, i.e.,
\[
f(x^k) - f(x^*)\le\frac{L\|x^0-x^*\|}{k+1}
\]
\end{theorem}
First prove a simple proposition:
\begin{proposition}\label{Pro:5:1}
A convex function $f\in\mathcal{C}^2$ with Lipschitz gradient constant $L$ has a bounded Hessian matrix:
\[
\nabla^2f(x)\preceq L\bm I
\]
\end{proposition}
\begin{proof}[Proof for proposition(\ref{Pro:5:1})]
Otherwise $\exists x_0,v$ such that
\[
\|\nabla^2f(x_0)v\|>L\|v\|.
\]
Thus we apply taylor expansion near $x_0$ for $x=x_0+v$:
\begin{align*}
\nabla f(x)&=\nabla f(x_0)+\nabla^2f(x_0)(x-x_0)+o(1)(x-x_0)
\end{align*}
It follows that 
\[
\|\nabla f(x)-\nabla f(x_0)\|= \|\nabla^2f(x_0)(x-x_0)+o(1)(x-x_0)\|\le L\|x-x_0\|
\]
Thus for sufficiently small $v$, we have $\|\nabla f(x)-\nabla f(x_0)\|\le L\|x-x_0\|$, which is a contradiction.
\end{proof}
\paragraph{Step 1: Apply Lipschitz condition}
We do the taylor expansion of $x^{k}$ for the point $x^{k+1}$:
\begin{align*}
f(x^{k+1})&=f(x^k)+\inp{\nabla f(x^k)}{x^{k+1} - x^k}+\frac{1}{2}(x^{k+1} - x^k)\trans\nabla^2 f(x^k+\tau (x^{k+1 - x^k}))(x^{k+1} - x^k)\\
&\le f(x^k)+\inp{\nabla f(x^k)}{x^{k+1} - x^k}+\frac{L}{2}\|x^{k+1} - x^k\|^2\\
&=f(x^k)-\frac{1}{2L}\|\nabla f(x^k)\|^2
\end{align*}
implying that
\[
\|\nabla f(x^k)\|^2\le2L[f(x^k) - f(x^{k+1})]
\]
and therefore 
\begin{equation}\label{Eq:5:5}
\sum_{k=0}^r\|\nabla f(x^k)\|^2\le
\sum_{k=0}^\infty \|\nabla f(x^k)\|^2\le
 2L[f(x^0) - f(x^{*})]
\le L^2\|x^0-x^*\|^2
\end{equation}
\paragraph{Step 2: Applying Convexity of $f$}
By the convexity of $f$ and the bound on its gradient, we can estimate the total error $\sum_{k=0}^r(f(x^k) - f(x^*))$:
\begin{align*}
\|x^{k+1} - x^*\|^2&=\|x^k-\frac{1}{L}\nabla f(x^k) - x^*\|^2\\
&=\|x^k-x^*\|^2-\frac{2}{L}\inp{\nabla f(x^k)}{x^k-x^*}+\frac{1}{L^2}\|\nabla f(x^k)\|^2\\
&\le \|x^k-x^*\|^2-\frac{2}{L}(f(x^k) - f(x^*))+\frac{1}{L^2}\|\nabla f(x^k)\|^2
\end{align*}
which implies
\begin{align*}
\sum_{k=0}^r f(x^k) - f(x^*)&\le \frac{L}{2}\sum_{k=0}^r
\left[
\|x^k-x^*\|-\|x^{k+1} - x^*\|^2\right]
+\frac{1}{2L}\sum_{k=0}^r\|\nabla f(x^k)\|^2\\
&\le  \frac{L}{2}\left[\|x^0-x^*\|^2-\|x^{r+1} - x^*\|^2\right]+\frac{L}{2}\|x^0-x^*\|^2\\&\le L\|x^0-x^*\|^2
\end{align*}
\paragraph{Step 3: applying monotonicity of $f(x^k) - f(x^*)$}
By the monotonicity of $f(x^k) - f(x^*)$,
\begin{align*}
f(x^r) - f(x^*)&\le\frac{1}{r+1}\sum_{k=0}^r f(x^k) - f(x^*)\\
&\le\frac{L\|x^0-x^*\|^2}{r+1}
\end{align*}
\begin{remark}
The convergence rate for this method is \emph{super-linear}. 
This upper bound is order-tight, i.e., we can find one example satisfying the equality.
\end{remark}
However, the constant step-size method can be faster if the given function $f$ is \emph{strongly convex}.



\begin{proposition}
The iteration $x^{k+1} = x^k-\frac{1}{L}\nabla f(x^k)$ for a strongly conex function $f\in\mathcal{C}^2$ with Lipschitz gradient gives linear convergence rate, i.e.,
\[
f(x^k) - f(x^*)=O(\rho^k), 
\]
with $\rho = 1-\frac{\sigma}{L}$.
\end{proposition}
\paragraph{Step 1: Lipschitz gradient gives upper bound on $f(x^{k+1}) - f(x^k)$}
Applying Taylor Expansion,
\begin{align*}
f(x^{k+1}) - f(x^k)&= \inp{\nabla f(x^k)}{x^{k+1}-x^k}+\frac{1}{2}(x^{k+1}-x^k)\trans\nabla^2f(x^k+\tau(x^{k+1}-x^k))(x^{k+1}-x^k)\\
&\le
\inp{\nabla f(x^k)}{x^{k+1}-x^k}+
\frac{L}{2}\|x^{k+1}-x^k\|^2
\end{align*}
Substituting $x^{k+1} -x^k=-\frac{1}{L}\nabla f(x^k)$, we derive:
\[
f(x^{k+1}) - f(x^k)\le-\frac{1}{2L}\|\nabla f(x^k)\|^2
\]
\paragraph{Step 2: Stronly convex gives upper bound on $f(x^*) - f(x)$}
For $\forall x,y$, we have
\begin{align*}
f(y)&=f(x)+\inp{\nabla f(x)}{y-x}+\frac{1}{2}(y-x)\trans\nabla^2f(z)(y-x)\\
&\ge f(x)+\inp{\nabla f(x)}{y-x}+\frac{\sigma}{2}\|y-x\|^2
\end{align*}
where $\min_i\lambda_i(\nabla^2 f(x))\ge\sigma>0,\forall $. Minimizing both sides in terms of $y$ gives
\[
f(x^*)\ge f(x)-\frac{1}{2\sigma}\|\nabla f(x)\|^2
\]

\paragraph{Step 3: Re-arrange bounds on $f(x^{k+1}) - f(x^*)$}
Applying those bounds, we derive
\begin{align*}
f(x^{k+1}) - f(x^*)&\le f(x^{k}) - f(x^*)-\frac{1}{2L}\|\nabla f(x^k)\|^2\\
&\le  f(x^{k}) - f(x^*)-\frac{1}{2L}\|\nabla f(x^k)\|^2\\
&\le  f(x^{k}) - f(x^*)-\frac{\sigma}{L}[f(x^k) - f(x^*)]\\
&=(1-\frac{\sigma}{L})[f(x^k) - f(x^*)]
\end{align*}
Applying this trick recursively,
\[
f(x^{k}) - f(x^*)\le(1-\frac{\sigma}{L})^k[f(x^0) - f(x^*)]
\]
Thus we have shown the linear convergence rate $f(x^k) - f(x^*)=O(\rho^k)$ with $\rho=1-\frac{\sigma}{L}$.


\paragraph{Reading Assignment: }
\begin{enumerate}
\item
Nesterov's Introductory Courses on Convex Programming
\item
Prof. Luo's note $\#4$.
\end{enumerate}











