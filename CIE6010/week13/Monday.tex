
\chapter{Week12}

\section{Monday}\index{week6_Tuesday_lecture}
\subsection{Comments on Final Project}

The final project aims to solve the optimization problem given below:
\begin{equation}\label{Eq:12:1}
\begin{array}{ll}
\min&f(x,y)=\frac{1}{2}(\bm x\trans\bm A\bm x+\bm y\trans\bm A\bm y) - \bm b\trans\bm x-\bm c\trans\bm y:=q_b(\bm x)+q_c(\bm y)\\
\mbox{such that}&h_1(\bm x,\bm y):=\frac{1}{2}(\bm x\trans\bm x-1)=0\\
&h_2(\bm x,\bm y):=\frac{1}{2}(\bm y\trans\bm y-1)=0\\
&h_3(\bm x,\bm y):=\bm x\trans\bm y=0
\end{array}
\end{equation}

The Lagrangian function is given by:
\begin{equation}
L(\bm x,\bm y,\lambda)=q_b(\bm x)+q_c(\bm y)+\sum_{i=1}^3h_i(\bm x,\bm y)\lambda_i
\end{equation}

The stationary point should satisfy:
\begin{align*}
\nabla_{\bm x}L=
\bm{Ax}-\bm b+\lambda_1\bm x+\lambda_3\bm y&=0\\
\nabla_{\bm y}L=
\bm{Ay}-\bm c+\lambda_2\bm y+\lambda_3\bm x&=0\\
\|h(\bm x,\bm y)\|^2&=0
\end{align*}

Based on the stationarity condition, the stopping criteria is given by:
\[
\max\left\{
\frac{\|\nabla_{\bm x}L\|}{\|\bm b\|+1},
\frac{\|\nabla_{\bm y}L\|}{\|\bm c\|+1},
\|h(\bm x)\|
\right\}\le\mbox{tol}
\]

The problem(\ref{Eq:12:1}) admits its global minimum since the constraint set is compact. Three methods are suggests: ADMM (the lecturer used this one); ALMM; quadratic penalty method.

\paragraph{Suggested ADMM method}
The key of ADMM is to separate variables in constraint. Thus to eliminate $h_3(\bm x,\bm y)=0$, define the second-kind Lagrangian function
\[
\hat L(\bm x,\bm y,\lambda) = q_b(\bm x)+q_c(\bm y)+\lambda_3\bm x\trans\bm y+\frac{h}{2}(\bm x\trans\bm y)^2
\]

It suffices to do the minimization
\begin{equation}
\begin{array}{ll}
\min&\hat L(\bm x,\bm y,\lambda)\\
&\bm x\in X=\{\bm x\mid \bm x\trans\bm x=1\}\\
&\bm y\in Y=\{\bm y\mid \bm y\trans\bm y=1\}\\
\end{array}
\end{equation}

The update rule for $\lambda_3$ is therefore given by ($\tau=1.618$)
\[
\lambda_3=\lambda_3+\tau\rho(\bm x\trans\bm y)
\]

To implement such a problem, you need to develop a solver for the \emph{trust region} sub-problem
\begin{equation}\label{Eq:12:4}
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm g\trans\bm p\\
&
\bm p\trans\bm p=\Delta^*
\end{array}
\end{equation}

How to get the \emph{global minimum} for the non-convex problem (\ref{Eq:12:4})? Unfortunately, solving this problem is at least in polynomial time in general, which is not acceptable.


\subsection{Trust Region Method}
However, our $\bm B$ for (\ref{Eq:12:4}) has the special form
\[
\bm A+\lambda_1\bm I+\rho\bm y\bm y\trans
\]
with sparse $\bm A$ and dense rank 1 matrix $\bm y$. The key to solve (\ref{Eq:12:4}) is to get the inverse of $\bm B$. It suffices to apply the Sherman-Morrison Formula, or apply congugate gradient (command: pcg)

\paragraph{Another application of Trust Region Problem}

Our goal is to minimize $\min_{\bm x\in\mathbb{R}^n}f(\bm x)$. To choose the step-size and the direction, we apply the trust region method. First approximate $f(x)$ with quadratic problem:
\begin{align*}
f(x^*+p)&\approx
f(x^*)+\nabla\trans f(x^*)p+\frac{1}{2}p\trans\nabla^2f(x^*)p\\
&\approx
f(x^*)+\nabla\trans f(x^*)p+\frac{1}{2}p\trans\underbrace{\bm B}_{\text{Approximate Hessian}}p:=m(p)
\end{align*}
It suffices to choose the step size $p$ to minimize the quadratic function above. The constraint is that the $\|p\|$ should be small enough. Thus it suffices to solve
\begin{equation}\label{Eq:12:5}
\begin{array}{ll}
\min&\frac{1}{2}\bm p\trans\bm B\bm p+\bm g\trans\bm p\\
&\|\bm p\|\le\Delta
\end{array}
\end{equation}

The general trust region descent method is proposed below:
\begin{enumerate}
\item
Solve the TR subproblem (\ref{Eq:12:5}) to get $p$
\item
$\rho:=\frac{f(x) - f(x+p)}{m(0) - m(p)}$. 
\begin{enumerate}
\item
If $\rho<\frac{1}{4}$, decrease the trust region $\Delta$ to $\frac{1}{4}\Delta$.
\item
If $\rho>\frac{3}{4}$, increase $\Delta$ into $2\Delta$
\item
Else, keep $\Delta$.
\end{enumerate}
\item
\begin{enumerate}
\item
If $\rho\ge\frac{1}{4}$ and $\rho>\eta$, then $x\leftarrow x+p$
\item
Else, $x$ keeps unchanged.
\end{enumerate}
\end{enumerate}




\section{Monday Tutorial}
\subsection{Sub-gradient}
It's defined for convex functions:
\begin{definition}[Sub-gradient]
The sub-gradient for a function $f$ is given by:
\[
\partial f(x)=\{g\mid g\trans (y-x)\le f(y) - f(x),\forall y\}
\]
\end{definition}
\begin{proposition}
$x^*$ minimizes $f(x)$ globally iff $0\in\partial f(x)$
\end{proposition}
\begin{proof}
$
f(y)\ge f(x^*)+0\trans(y-x^*)
$
\end{proof}
\begin{remark}
$-\partial f(x)$ is not necessarily a descent direction; while $-\nabla f(x)$ is a descent direction. For example, $f(x,y)=|x|+2|y|$, and $g=(1,2)\in\partial f(1,0)$, but $(-1,-2)$ is not descent direction.
\end{remark}

\begin{example}
For $f(x) = \|x\|_2^2$, its subgradient is
\[
\partial f(x)=\left\{
\begin{aligned}
\frac{x}{\|x\|_2},&\quad x\ne0\\
\{v:\|v\|_2\le 1\},&\quad x=0
\end{aligned}
\right.
\]
For $f(x)=\|x\|_1$, its subgradient is $\partial f(x)=J_1\times\cdots\times J_n$, with
\[
J_i=\left\{
\begin{aligned}
1,&x_i>0\\
-1,&x_i<0\\
[-1,1],&x_i=0
\end{aligned}
\right.
\]
\end{example}
\paragraph{Proximal operator}
During the ADMM implementation, you may apply the proximal operator, which aims to solve the optimization problem
\[
\mbox{prox}_f(x)=\arg\min_{u}f(u)+\frac{1}{2}\|u-x\|^2_2
\]
When $f=\|\cdot\|_1$, it suffices to solve
\[
\arg\min_{u}\|u\|_1+\frac{1}{2}\|u-x\|_2^2
\]

When $f=\|\cdot\|_1$, the solution is soft threshold.
\paragraph{One-Norm Proximal operator}

Given the problem
\[
\min\frac{1}{2}\|Ax-b\|_2^2+\lambda\|x\|_1
\]
\begin{enumerate}
\item
Split the problem into
\[
\begin{array}{ll}
\min&\frac{1}{2}\|Ax-b\|_2^2+\lambda\|z\|_1\\
&x=z
\end{array}
\]
\item
The second step is to write down the augmented Lagrangian:
\[
L_{\rho}=\frac{1}{2}\|Ax-b\|_2^2+\lambda\|z\|_1+y\trans(x-z)+\frac{\rho}{2}\|x-z\|_2^2
\]
\item
Do the minimization
\begin{align*}
x^{k+1}&=\arg\min_{x}L_\rho(x,y^k,z^k)\\
&=(A\trans A+\rho I)^{-1}(A\trans b + \rho z^k - y^k)\\
z^{k+1}&=\arg\min\lambda\|z\|_1+\frac{\rho}{2}\left\|x^{k+1}+\frac{y^k}{\rho} - z\right\|_2^2\\
&=\mbox{prox}_{\lambda/\rho \|\cdot\|}(x^{k+1} + y^k/\rho)\\
&=S_{\lambda/\rho}(\underbrace{x^{k+1} + y^k/\rho}_{u})=\mbox{sign}(u)\cdot(|u| - \lambda/\rho)_+\\
y^{k+1} &= y^k+\rho(x^{k+1} - z^{k+1})
\end{align*}



\end{enumerate}










