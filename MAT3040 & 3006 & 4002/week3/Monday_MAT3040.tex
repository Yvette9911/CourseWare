
\chapter{Week3}
\section{Monday for MAT3040}\index{Monday_lecture}
\paragraph{Reviewing}
\begin{enumerate}
\item
Complementation. Suppose $\dim(V)=n<\infty$, then $W\le V$ implies $\exists W'$ such that
\[
W\oplus W'=V.
\]
\item
Given the linear transformation $T:V\to W$, define the set  $\ker(T)$ and $\text{Im}(T)$
\item
Isomorphism of vector spaces:
\[
T:V\cong W
\]
\item
Rank-Nullity Theorem
\end{enumerate}
\begin{remark}
On isomorphism $T$ on vector spaces,
\begin{enumerate}
\item
The set $\{\bm v_1,\dots,\bm v_k\}$ is linearly independent in $V$ if and only if $\{T\bm v_1,\dots,T\bm v_k\}$ is linearly independent.

Question 8 in Homework 1: if $T:V\to W$ is injective, then $\{\bm v_1,\dots,\bm v_k\}$ is linearly independent in $V$ implies $\{T\bm v_1,\dots,T\bm v_k\}$ is linearly independent.
\item
The same goes if we replace the linearly independence by spans.
\item
If $\dim(V)=n$, then $\{\bm v_1,\dots,\bm v_n\}$ forms a basis of $V$ if and only if $\{T\bm v_1,\dots,T\bm v_n\}$ forms a basis of $W$. In particular, $\dim(V)=\dim(W)$.

Actually, if $\dim(V)=\dim(W)=n$, then $V\cong W$.
\end{enumerate}
\end{remark}

\subsection{Change of Basis and Matrix Representation}

\begin{definition}
Let $V$ be a finite dimensional vector space and $B=\{\bm v_1,\dots,\bm v_n\}$ an \emph{ordered} basis of $V$. The \emph{coordinate vector} of a vector $\bm v\in V$ is given by:
\[
\bm v=\alpha_1\bm v_1+\cdots+\alpha_n\bm v_n
\implies
\mapsto [\bm v]_B=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}
\]
\end{definition}
Note that $\{\bm v_1,\bm v_2,\dots,\bm v_n\}\ne\{\bm v_2,\bm v_1,\dots,\bm v_n\}$ w.r.t. ordered basis.

\begin{example}
Given $V=M_{2\times 2}(\mathbb{F})$ and
\[
B=
\left\{
\begin{pmatrix}
1&0\\0&0
\end{pmatrix},
\begin{pmatrix}
0&1\\0&0
\end{pmatrix},
\begin{pmatrix}
0&0\\1&0
\end{pmatrix},
\begin{pmatrix}
0&0\\0&1
\end{pmatrix},
\right\}
\]
any matrix has the coordinate vector w.r.t. $B$:
\[
\left[
\begin{pmatrix}
1&4\\2&3
\end{pmatrix}
\right]_B
=
\begin{pmatrix}
1\\4\\2\\3
\end{pmatrix}
\]
Howver, given another ordered basis
\[
B=
\left\{
\begin{pmatrix}
0&1\\0&0
\end{pmatrix},
\begin{pmatrix}
1&0\\0&0
\end{pmatrix},
\begin{pmatrix}
0&0\\1&0
\end{pmatrix},
\begin{pmatrix}
0&0\\0&1
\end{pmatrix},
\right\}
\]
any matrix has the coordinate vector w.r.t. $B_1$:
\[
\left[
\begin{pmatrix}
1&4\\2&3
\end{pmatrix}
\right]_{B_1}
=
\begin{pmatrix}
4\\1\\2\\3
\end{pmatrix}
\]
\end{example}

\begin{theorem}
The mapping 
\[
[]_B:V\mapsto\mathbb{F}^n
\]
is an isomorphism of vector spaces.
\end{theorem}
\begin{proof}
\begin{enumerate}
\item
The operator $[]_B$ is well-defined: suppose
\[
[\bm v]_B=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix},\qquad
[\bm v]_B=\begin{pmatrix}
\alpha_1'\\\vdots\\\alpha_n'
\end{pmatrix},
\]
then 
\begin{align*}
\bm v&=\alpha_1\bm v_1+\cdots+\alpha_n\bm v_n\\
&=\alpha_1'\bm v_1+\cdots+\alpha_n'\bm v_n
\end{align*}
By the uniqueness of coordinates, we imply $\alpha_i=\alpha_i'$ for $i=1,\dots,n$.
\item
The operator $[]_B$ is a linear transformation, i.e., 
\[
[p\bm v+q\bm w]_B=p[\bm v]_B+q[\bm w]_B,\qquad
p,q\in\mathbb{F}
\]
\item
The operator $[]_B$ is surjective: suppose
\[
[\bm v]_B=\begin{pmatrix}
0\\\vdots\\0
\end{pmatrix}\in\mathbb{F}^n,
\]
then $\bm v=0\bm v_1+\cdots+0\bm v_n=\bm0$.
\item
The injective is clear.


\end{enumerate}
Therefore, $[]_B$ is an isomorphism.
\end{proof}

Exercise: if $\dim(V)=\dim(W)$, and $T:V\to W$ is injective, then $V\cong W$.

\begin{example}
For $V=P_3[x]$ and its basis $B=\{1,x,x^2,x^3\}$.

To check if the set $\{1+x^2,3-x^3,x-x^3\}$ is linearly independent, it suffices to check the corresponding coordinate vectors
\[
\left\{
\begin{pmatrix}
1\\0\\1\\0
\end{pmatrix},
\begin{pmatrix}
3\\0\\0\\-1
\end{pmatrix},
\begin{pmatrix}
0\\1\\0\\-1
\end{pmatrix}
\right\}
\]
is linearly independent (Do Gaussian Elimination and check the number of pivots).
\end{example}

\begin{example}
Questions: if $B_1,B_2$ form two basis of $V$, then how are $[\bm v]_{B_1},[\bm v]_{B_2}$ related to each other.

Suppose $V=\mathbb{R}^n$ and $B_1=\{\bm e_1,\dots,\bm e_n\}$. For any $\bm v\in V$,
\[
\bm v=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}=\alpha_n\bm e_1+\cdots+\alpha_n\bm e_n
\implies
[\bm v]_{B_1}=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}
\]
Also, it is clear that the $B_2$ forms a basis as well:
\[
B_2=
\left\{
\begin{pmatrix}
1\\0\\\vdots\\0
\end{pmatrix},
\begin{pmatrix}
1\\1\\\vdots\\0
\end{pmatrix},
\ldots,
\begin{pmatrix}
1\\1\\\vdots\\1
\end{pmatrix}
\right\}
\]
which implies
\[
[\bm v]_{B_2}=
\begin{pmatrix}
\alpha_1-\alpha_2\\
\alpha_2-\alpha_3\\
\vdots\\
\alpha_{n-1}-\alpha_n\\
\alpha_n
\end{pmatrix}
\]
\end{example}

\begin{proposition}[Change of Basis]
Let $A=\{\bm v_1,\dots,\bm v_n\}$ and $A'=\{\bm w_1,\dots,\bm w_n\}$ be two basis of a vector space $V$.
Suppose $\bm v_j=\sum_{i=1}^n\alpha_{ij}\bm w_i$ for $j=1,\dots,n$. Then the \emph{change of basis} matrix
\[
\mathcal{C}_{A',A}=\begin{pmatrix}
\alpha_{ij}
\end{pmatrix}_{i,j=1,\dots,n}
\]
satisfies the following:
\begin{equation}\label{Eq:3:1}
\mathcal{C}_{A',A}[\bm v]_A=[\bm v]_{A'}
\end{equation}
Also, the matrix $\mathcal{C}_{A',A}$ is invertible with the inverse
\[
(\mathcal{C}_{A',A})^{-1}=\mathcal{C}_{A,A'}
\]
where $\mathcal{C}_{A,A'}=(\beta_{ij})$, with $\beta_{ij}$ satisfying
\[
\bm w_j=\sum_{i=1}^n\beta_{ij}\bm v_i
\]
\end{proposition}

\begin{proof}
Consider $\bm v=\bm v_j$, then LHS of~(\ref{Eq:3:1}) is
\[
(\alpha_{ij})\bm e_j=\begin{pmatrix}
\alpha_{1j}\\\vdots\\\alpha_{nj}
\end{pmatrix}
\]
the RHS of (\ref{Eq:3:1}) is 
\[
[\bm v_j]_{A'}=[\sum_{i=1}^n\alpha_i\bm w_i]_{A'}
=
\begin{pmatrix}
\alpha_{1j}\\\vdots\\\alpha_{nj}
\end{pmatrix}
=\text{LHS}
\]
Therefore, $\mathcal{C}_{A',A}[\bm v_j]_A=[\bm v_j]_{A'}$ for $\forall j=1,\dots,n$.

Then for all $\bm v=r_1\bm v_1+\cdots+r_n\bm v_n$,
\begin{align*}
\mathcal{C}_{A',A}[\bm v]_{A}&=\mathcal{C}_{A',A}[r_1\bm v_1+\cdots+r_n\bm v_n]_{A}\\
&=\mathcal{C}_{A',A}
\left[
r_1[\bm v_1]_A+\cdots+r_n[\bm v_n]_A
\right]\\
&=\sum_{j=1}^nr_j\mathcal{C}_{A',A}[\bm v_j]_A\\
&=\sum_{j=1}^nr_j[\bm v_j]_{A'}\\
&=\left[\sum_{j=1}^nr_j\bm v_j\right]_{A'}\\
&=(\bm v)_{A'}
\end{align*}

Noew, suppose 
\begin{align*}
\bm v_j&=\sum_{i=1}^n\alpha_{ij}\bm w_i\\
&=\sum_{i=1}^n\alpha_{ij}\sum_{k=1}^n\beta_{ki}\bm v_k\\
&=\sum_{k=1}^n\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)\bm v_i
\end{align*}
By the uniqueness of coordinates, we imply
\[
\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)
=
\left\{
\begin{aligned}
1,&\quad j=k\\
0,&\quad j\ne k
\end{aligned}
\right.
\]
where
\[
\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)
=\left(
\mathcal{C}_{AA'}\mathcal{C}_{A'A}
\right)
\]
Therefore, $\left(
\mathcal{C}_{AA'}\mathcal{C}_{A'A}
\right)=\bm I_n$.
\end{proof}
\begin{example}
Back to Example~(3.3), suppose
\[
B_1=\{\bm e_1,\dots,\bm e_n\},
\qquad
B_2=\{\bm w_1,\dots,\bm w_n\}
\]
and suppose $\bm w_i=\bm e_1+\cdots+\bm e_i$. Therefore,
\[
\mathcal{C}_{B_1,B_2}=\begin{pmatrix}
1&1&\cdots&1\\
0&1&\cdots&1\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&1
\end{pmatrix}
\]
and
\[
\mathcal{C}_{B_1,B_2}[\bm v]_{B_2}
=
\begin{pmatrix}
1&1&\cdots&1\\
0&1&\cdots&1\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&1
\end{pmatrix}
\begin{pmatrix}
\alpha_1-\alpha_2\\\vdots\\\alpha_{n-1}-\alpha_n\\\alpha_n
\end{pmatrix}=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}=[\bm v]_{B_2}
\]
\end{example}

\begin{definition}
Let $T:V\to W$ be a linear transformation, and 
\[
\begin{array}{ll}
\mathcal{A}=\{\bm v_1,\dots,\bm v_m\},
&
\mathcal{B}=\{\bm w_1,\dots,\bm w_m\}
\end{array}
\]
be bases of $V$ and $W$, respectively. 
The \emph{matrix representation} of $T$ with respect to (w.r.t.) $\mathcal{A}$ and $\mathcal{B}$ is given by:
\[
T(\bm v_j)=\sum_{i=1}^m\alpha_{ij}\bm w_j
\implies
(T)_{\mathcal{B}\mathcal{A}}
=
(\alpha_{ij})_{i,j=1,\dots,m},
\]
where $(T)_{\mathcal{B}\mathcal{A}}\in M_{m\times m}(\mathbb{F})$.
\end{definition}











