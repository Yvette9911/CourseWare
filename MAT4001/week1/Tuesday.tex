
\chapter{Week1}

\section{Monday}\index{Monday_lecture}
\subsection{Introduction to Numerical Analysis}
\paragraph{Solving Nonlinear Equations}
For example, we want to solve a nonlinear equation $w(1)$ with $w$ to be the LambertW function:
\[
we^w = 1.
\]
This topic will be taught in chapter 2.

\paragraph{Interpolation}
Given a list of data points, our aim is to recover/approximate the origin function over a function class, i.e., piecewise linear functions or polynomials. This topic will be taught in chapter 3.

\paragraph{Numerical Integration}
The cdf of the standard normal distribution is given by:
\[
N(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^x\exp\left(-\frac{t^2}{2}\right)\diff t
\]
To approximate the values for $N(x)$, we need to apply a numerical method (quadrature rules) to evaluate. This topic will be taught in chapter 4.

\paragraph{Solving Linear Systems}
To find the solutions of a linear system of equations 
\[
\bm{Ax}=\bm b,
\]
e.g., when we use \textit{finite difference} method to solve a differential equation, it is necessary to apply some numerical method to solve it in computer. This topic will be taught in chapter 5.

\paragraph{Least Squares}
If we have more time, we will teach how to fit a set of data points by a function from a function class.

\subsection{Basic Concepts}
\begin{definition}[Truncation Error]
The error made by numerical algorithms that arises from taking finite number of steps in computation
\end{definition}
For example, consider the taylor's theorem
\[
f(x)=P_n(x)+R_n(x)
\]
where
\begin{align*}
P_n(x)&=f(x_0)+f'(x_0)(x-x_0)+\cdots+\frac{f^{(n))}(x_0)}{n!}(x-x_0)^n\\
R_n(x)&=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}
\end{align*}
If we use $P_n(x)$ to approximate $f(x)$, the error $R_n(x)$ is called the \emph{truncation error}.

\begin{definition}[Round-off Error]
The error produced when a computer is used to
perform real number calculations, i.e., the computer only gives approximate value for some real numebers.
\end{definition}

For example, for numbers $\frac{1}{3},\pi,\sqrt{2}$, they cannot be represented exactly in the computation by a computer.

Such errors are invertible, but  we can try to minimize the negative impact of these errors by rewriting the formula that are wish to compute.

\begin{definition}[Binary Floating Point Number]
A 64-bit (binary digit) representation is used for a real number:
\[
(-1)^s2^{c-1023}(1+f)
\]
\end{definition}




\begin{definition}
Suppose $p^*$ is an approximation to $p$. The acutal error is $p-p^*$, the absolute error is $|p-p^*|$, and the relative error is $\frac{|p-p^*|}{|p|}$ provided that $p\ne0$.
\end{definition}
\begin{definition}
Suppose $p^*$ is an approximation to $p$, then $p^*$ is said to approximate $p$ to $t$ \emph{significant digits} if $t$ is the largest nonnegative integer for which
\[
\frac{|p-p^*|}{|p|}\le5\times 10^{-t}.
\]
\end{definition}


\subsection{Convergence and Stability}
\begin{definition}[Convergence]
Suppose a sequence $\{\beta_n\}_{n=1}^\infty$ is known to converge to zero, and $\{\alpha_n\}_{n=1}^\infty$ converges to a number $\alpha$. If there exists $K>0$ such that
\[
\begin{array}{ll}
|\alpha_n - \alpha|\le K|\beta_n|
&
\mbox{for large $n$,}
\end{array}
\]
then $\{\alpha_n\}_{n=1}^\infty$ is said to converge to $\alpha$ with rate of convergence $\mathcal{O}(\beta_n)$, which is denoted as
\[
\alpha_n = \alpha + \mathcal{O}(\beta_n).
\]
\end{definition}











